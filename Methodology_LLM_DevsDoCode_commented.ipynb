{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated Methodology for Generating Individuals in Document Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miro: https://miro.com/app/board/uXjVI7yFd2Y=/\n",
    "\n",
    "# Methodology developed by the Portuguese consortium, designed to enable the extraction of ontology individuals from documents in a faster and more efficient way.\n",
    "# This methodology is based on a free GPT API called DevsDoCode (t.me/devsdocode) and implemented using Python.\n",
    "\n",
    "# The methodology needs some specific improvements, but for the initial idea it is already fully functional. The code is provided as a starting point for further development and improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract PDF to word using ILovePDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The content is converted from PDF to Word to facilitate automated information extraction, while ensuring the preservation of the original table structure and formatting. For this reason, a web-based converter was used instead of Python libraries, as the latter often distorted the layout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fire-Safety Document Analyzer – Country, Language and Standards Extraction Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script reads a .docx file containing technical or legal text, sends the content to a GPT API that identifies the country, language, and the possible fire safety standards. The user can view these standards in a graphical interface, select the relevant standard for analysis, manually add new ones, and save the final selection to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import textwrap\n",
    "import tkinter as tk\n",
    "import customtkinter as ctk\n",
    "from tkinter import messagebox\n",
    "import docx\n",
    "from openai import OpenAI\n",
    "\n",
    "# ─────────── OpenAI Configuration ───────────\n",
    "# Initialize OpenAI client\n",
    "CLIENT = OpenAI(\n",
    "    api_key=\"ddc-temp-free-e3b73cd814cc4f3ea79b5d4437912663\",\n",
    "    base_url=\"https://api.devsdocode.com/v1\",\n",
    ")\n",
    "\n",
    "# ─────────── File Paths ───────────\n",
    "# Input Word document and output JSON file paths\n",
    "DOCX_INPUT = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\Documento_Nativo.docx\"\n",
    "OUTPUT_JSON = DOCX_INPUT.replace(\".docx\", \"_results_gui.json\")\n",
    "\n",
    "# ─────────── Helper Functions ───────────\n",
    "def extract_docx_text(path: str) -> str:\n",
    "    \"\"\"Extract text from a Word document.\"\"\"\n",
    "    try:\n",
    "        doc = docx.Document(path)\n",
    "        return \"\\n\".join(paragraph.text.strip() for paragraph in doc.paragraphs if paragraph.text.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Word document: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def query_gpt(prompt: str, retries=3, temperature=0):\n",
    "    \"\"\"Query OpenAI API and return response.\"\"\"\n",
    "    wait = 6\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = CLIENT.chat.completions.create(\n",
    "                model=\"provider-4/gpt-4.1\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"GPT error (attempt {attempt+1}/{retries}): {e}\")\n",
    "            time.sleep(wait)\n",
    "            wait *= 2\n",
    "    return \"\"\n",
    "\n",
    "def clean_json(text: str):\n",
    "    \"\"\"Sanitize and parse GPT output into valid JSON.\"\"\"\n",
    "    # Remove code fences and extra whitespace\n",
    "    text = re.sub(r\"```(?:json)?(.*?)```\", r\"\\1\", text, flags=re.DOTALL).strip()\n",
    "    text = re.sub(r\"^json\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "    try:\n",
    "        # Extract JSON object\n",
    "        text = text[text.index(\"{\"):text.rindex(\"}\")+1]\n",
    "        # Fix common JSON errors\n",
    "        text = re.sub(r\",\\s*}\", \"}\", text)\n",
    "        text = re.sub(r\",\\s*]\", \"]\", text)\n",
    "        return json.loads(text)\n",
    "    except Exception as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        return {\n",
    "            \"country\": \"Unknown\",\n",
    "            \"language\": \"??\",\n",
    "            \"legal_regulations\": [],\n",
    "            \"technical_standards\": [],\n",
    "            \"other_documents\": []\n",
    "        }\n",
    "\n",
    "def shorten_text(text: str, max_words=30, max_chars=200):\n",
    "    \"\"\"Shorten text for display.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Capitalize first letter\n",
    "    text = text[0].upper() + text[1:]\n",
    "    # Truncate by words\n",
    "    if len(text.split()) > max_words:\n",
    "        text = \" \".join(text.split()[:max_words]) + \"…\"\n",
    "    # Truncate by characters\n",
    "    if len(text) > max_chars:\n",
    "        text = text[:max_chars].rsplit(\" \", 1)[0] + \"…\"\n",
    "    # Ensure proper punctuation\n",
    "    if text[-1] not in \".!?\":\n",
    "        text += \".\"\n",
    "    return text\n",
    "\n",
    "# ─────────── Prompt Template ───────────\n",
    "# Prompt for extracting fire-safety standards\n",
    "PROMPT_TEMPLATE = \"\"\"You are an international expert in fire-safety legislation.\n",
    "\n",
    "Analyze the excerpt below and return:\n",
    "1. \"country\" – detected country\n",
    "2. \"language\" – ISO-2 code\n",
    "3. \"standards\" – only the official fire-safety documents explicitly cited.\n",
    "Group them in:\n",
    " • \"legal_regulations\"\n",
    " • \"technical_standards\"\n",
    " • \"other_documents\"\n",
    "\n",
    "Each entry: { \"name\": \"...\", \"country\": \"<same>\", \"justification\": \"<≤120 words>\" }\n",
    "\n",
    "Return ONLY this JSON (no markdown):\n",
    "\n",
    "{\n",
    " \"country\": \"...\",\n",
    " \"language\": \"...\",\n",
    " \"legal_regulations\": [...],\n",
    " \"technical_standards\": [...],\n",
    " \"other_documents\": [...]\n",
    "}\n",
    "\n",
    "Excerpt:\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\"\"\"\n",
    "\n",
    "# ─────────── GPT Processing ───────────\n",
    "def detect_and_list_standards(text: str):\n",
    "    \"\"\"Split text and query GPT for fire-safety standards.\"\"\"\n",
    "    MAX_CHUNK_SIZE = 15000\n",
    "    # Split text into chunks if too long\n",
    "    chunks = textwrap.wrap(text, MAX_CHUNK_SIZE) if len(text) > MAX_CHUNK_SIZE else [text]\n",
    "    aggregated = {\n",
    "        \"legal_regulations\": [],\n",
    "        \"technical_standards\": [],\n",
    "        \"other_documents\": []\n",
    "    }\n",
    "    country = language = \"Unknown\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Query GPT and clean response\n",
    "        data = clean_json(query_gpt(PROMPT_TEMPLATE.format(chunk=chunk)))\n",
    "        country = data.get(\"country\", country)\n",
    "        language = data.get(\"language\", language)\n",
    "        # Aggregate standards\n",
    "        for key, value in data.items():\n",
    "            if key in aggregated:\n",
    "                for item in value:\n",
    "                    item[\"justification\"] = shorten_text(item.get(\"justification\", \"\"))\n",
    "                    aggregated[key].append(item)\n",
    "\n",
    "    # Remove duplicates\n",
    "    for key in aggregated:\n",
    "        seen = set()\n",
    "        unique_list = []\n",
    "        for item in aggregated[key]:\n",
    "            if item[\"name\"] not in seen:\n",
    "                unique_list.append(item)\n",
    "                seen.add(item[\"name\"])\n",
    "        aggregated[key] = unique_list\n",
    "\n",
    "    return {\n",
    "        \"country\": country,\n",
    "        \"language\": language,\n",
    "        \"standards\": aggregated\n",
    "    }\n",
    "\n",
    "# ─────────── GUI ───────────\n",
    "ctk.set_default_color_theme(\"blue\")\n",
    "ctk.set_appearance_mode(\"light\")\n",
    "\n",
    "class FireSafetyGUI(ctk.CTk):\n",
    "    \"\"\"GUI for displaying and interacting with detected fire-safety standards.\"\"\"\n",
    "    FONT_LARGE = (\"Segoe UI\", 17)\n",
    "    FONT_TEXT = (\"Segoe UI\", 13)\n",
    "\n",
    "    def __init__(self, data):\n",
    "        super().__init__(fg_color=\"#F5F6FA\")\n",
    "        self.data = data\n",
    "        self.title(\"Document Analyzer\")\n",
    "        self.geometry(\"1000x680\")\n",
    "\n",
    "        # Header\n",
    "        header = ctk.CTkFrame(self, fg_color=\"#e9ecef\")\n",
    "        header.pack(fill=\"x\", padx=22, pady=12)\n",
    "        ctk.CTkLabel(header, text=\"Detected Country:\", font=self.FONT_LARGE).grid(row=0, column=0, padx=12, sticky=\"w\")\n",
    "        ctk.CTkLabel(header, text=data[\"country\"], font=self.FONT_LARGE, text_color=\"#0d6efd\").grid(row=0, column=1, sticky=\"w\")\n",
    "        ctk.CTkLabel(header, text=\"Language:\", font=self.FONT_LARGE).grid(row=0, column=2, padx=(40, 4))\n",
    "        ctk.CTkLabel(header, text=data[\"language\"], font=self.FONT_LARGE, text_color=\"#0d6efd\").grid(row=0, column=3, sticky=\"w\")\n",
    "\n",
    "        # Tab view for standards\n",
    "        self.tab_view = ctk.CTkTabview(self, corner_radius=8)\n",
    "        self.tab_view.pack(fill=\"both\", expand=True, padx=22, pady=8)\n",
    "        self.checkboxes = []\n",
    "        for section, title in [\n",
    "            (\"legal_regulations\", \"Legal Regulations\"),\n",
    "            (\"technical_standards\", \"Technical Standards\"),\n",
    "            (\"other_documents\", \"Other Documents\")\n",
    "        ]:\n",
    "            self._populate_section(self.tab_view.add(title), data[\"standards\"][section])\n",
    "\n",
    "        # Manual addition frame\n",
    "        add_frame = ctk.CTkFrame(self)\n",
    "        add_frame.pack(fill=\"x\", padx=22)\n",
    "        self.manual_entry = tk.StringVar()\n",
    "        ctk.CTkEntry(\n",
    "            add_frame,\n",
    "            textvariable=self.manual_entry,\n",
    "            placeholder_text=\"➕ Add regulation manually\",\n",
    "            width=600,\n",
    "            font=self.FONT_TEXT\n",
    "        ).pack(side=\"left\", padx=6, pady=10)\n",
    "        ctk.CTkButton(add_frame, text=\"Add\", command=self.add_manual, width=120).pack(side=\"left\", padx=6)\n",
    "\n",
    "        # Confirm button\n",
    "        ctk.CTkButton(\n",
    "            self,\n",
    "            text=\"Confirm Selection\",\n",
    "            command=self.save,\n",
    "            font=(\"Segoe UI\", 15, \"bold\"),\n",
    "            fg_color=\"#198754\",\n",
    "            hover_color=\"#157347\"\n",
    "        ).pack(pady=14)\n",
    "\n",
    "    def _populate_section(self, parent, items):\n",
    "        \"\"\"Populate a section with standard items.\"\"\"\n",
    "        for idx, item in enumerate(items):\n",
    "            if not {\"name\", \"country\"}.issubset(item):\n",
    "                continue\n",
    "            var = ctk.BooleanVar(value=False)  # Default: unselected\n",
    "            card = ctk.CTkFrame(parent, fg_color=\"#ffffff\", corner_radius=6, border_width=1, border_color=\"#dfe3e6\")\n",
    "            card.pack(fill=\"x\", padx=8, pady=5)\n",
    "            ctk.CTkCheckBox(\n",
    "                card,\n",
    "                text=f\"{item['name']} ({item['country']})\",\n",
    "                variable=var,\n",
    "                font=self.FONT_LARGE\n",
    "            ).pack(anchor=\"w\", padx=10, pady=(6, 2))\n",
    "            if item.get(\"justification\"):\n",
    "                wrap_length = max(self.winfo_screenwidth() - 250, 940)\n",
    "                ctk.CTkLabel(\n",
    "                    card,\n",
    "                    text=item['justification'],\n",
    "                    font=self.FONT_TEXT,\n",
    "                    text_color=\"#6c757d\",\n",
    "                    wraplength=wrap_length\n",
    "                ).pack(anchor=\"w\", padx=36, pady=(0, 8))\n",
    "            self.checkboxes.append((var, item))\n",
    "\n",
    "    def add_manual(self):\n",
    "        \"\"\"Add a manually entered regulation.\"\"\"\n",
    "        name = self.manual_entry.get().strip()\n",
    "        if not name:\n",
    "            return\n",
    "        if any(name.lower() == item[\"name\"].lower() for _, item in self.checkboxes):\n",
    "            messagebox.showinfo(\"Duplicate\", \"This regulation is already listed.\")\n",
    "            return\n",
    "        item = {\"name\": name, \"country\": self.data[\"country\"], \"justification\": \"\"}\n",
    "        self._populate_section(self.tab_view.tab(\"Other Documents\"), [item])\n",
    "        self.manual_entry.set(\"\")\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"Save selected regulations to a JSON file.\"\"\"\n",
    "        selected = [item for var, item in self.checkboxes if var.get()]\n",
    "        if not selected:\n",
    "            messagebox.showwarning(\"Warning\", \"Please select at least one document.\")\n",
    "            return\n",
    "        output_data = {\n",
    "            \"country\": self.data[\"country\"],\n",
    "            \"language\": self.data[\"language\"],\n",
    "            \"selected_norms\": [{k: v for k, v in d.items() if k != \"country\"} for d in selected]\n",
    "        }\n",
    "        try:\n",
    "            with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "            messagebox.showinfo(\"Saved\", f\"Saved to:\\n{OUT_JSON}\")\n",
    "            self.destroy()\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to save JSON: {e}\")\n",
    "\n",
    "# ─────────── Main Execution ───────────\n",
    "def main():\n",
    "    \"\"\"Main function: read document, query GPT, and launch GUI.\"\"\"\n",
    "    # Check if input file exists\n",
    "    if not os.path.exists(DOCX_INPUT):\n",
    "        print(f\"Error: Input file {DOCX_INPUT} not found.\")\n",
    "        return\n",
    "\n",
    "    # Extract text from Word document\n",
    "    text = extract_docx_text(DOCX_INPUT)\n",
    "    if not text:\n",
    "        print(\"Error: No text extracted from document.\")\n",
    "        return\n",
    "\n",
    "    print(\"▶️ Querying model…\")\n",
    "    # Detect standards using GPT\n",
    "    data = detect_and_list_standards(text)\n",
    "    if not any(data[\"standards\"].values()):\n",
    "        print(\"⚠️ Model returned no standards.\")\n",
    "        return\n",
    "\n",
    "    # Launch GUI\n",
    "    FireSafetyGUI(data).mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Table Extraction and Normalization from DOCX Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzes the Word file and automatically extracts all table data, identifying titles and content, including handling of continued tables when applicable.\n",
    "# The extracted information is structured and saved as a JSON file, ready for further processing or validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from docx import Document\n",
    "from docx.table import Table\n",
    "from docx.text.paragraph import Paragraph\n",
    "from itertools import zip_longest\n",
    "\n",
    "# ─────────── File Paths ───────────\n",
    "# Input Word document and output JSON file paths\n",
    "SOURCE_DOCX = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\Documento_Nativo.docx\"\n",
    "DESTINATION_JSON = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\extracted_tables\\Tables_clean.json\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(DESTINATION_JSON), exist_ok=True)\n",
    "\n",
    "# ─────────── Regex Patterns ───────────\n",
    "# Match table titles with Roman numerals or digits\n",
    "TABLE_TITLE_REGEX = re.compile(r\"(?i)^(table|quadro)\\s+([ivxlcdm\\d]+)\", re.IGNORECASE)\n",
    "# Match dots separator (e.g., \" . . . \")\n",
    "DOTS_SEPARATOR = re.compile(r\"\\s*\\.\\s*\\.\\s*\\.\\s*\")\n",
    "# Match line breaks\n",
    "LINE_BREAKS = re.compile(r\"\\s*\\n\\s*\")\n",
    "\n",
    "# ─────────── Helper Functions ───────────\n",
    "def get_cell_text(cell):\n",
    "    \"\"\"Extract text from a table cell.\"\"\"\n",
    "    return \"\\n\".join(paragraph.text.strip() for paragraph in cell.paragraphs if paragraph.text.strip())\n",
    "\n",
    "def split_into_chunks(text):\n",
    "    \"\"\"Split text into chunks based on dots or line breaks.\"\"\"\n",
    "    # Priority: 1) dots separator, 2) line breaks, 3) whole text\n",
    "    if DOTS_SEPARATOR.search(text):\n",
    "        return [chunk.strip(\" .\") for chunk in DOTS_SEPARATOR.split(text) if chunk.strip(\" .\")]\n",
    "    if LINE_BREAKS.search(text):\n",
    "        return [chunk.strip() for chunk in LINE_BREAKS.split(text) if chunk.strip()]\n",
    "    return [text.strip()]\n",
    "\n",
    "def expand_row(cells):\n",
    "    \"\"\"Expand a row into multiple rows if cells contain multiple chunks.\"\"\"\n",
    "    parts = [split_into_chunks(cell) for cell in cells]\n",
    "    # If all cells have one chunk, return original row\n",
    "    if all(len(part) == 1 for part in parts):\n",
    "        return [cells]\n",
    "    # Create new rows from combinations of chunks\n",
    "    rows = []\n",
    "    for combo in zip_longest(*parts, fillvalue=\"\"):\n",
    "        rows.append(list(combo))\n",
    "    return rows\n",
    "\n",
    "def is_table_continuation(prev_table, current_table, paragraphs_between):\n",
    "    \"\"\"Check if current_table continues prev_table.\"\"\"\n",
    "    if not prev_table or not paragraphs_between:\n",
    "        return False\n",
    "    # Consider only significant paragraphs (non-empty and >10 chars)\n",
    "    significant_paras = [p for p in paragraphs_between if p.strip() and len(p.strip()) > 10]\n",
    "    return len(significant_paras) == 0\n",
    "\n",
    "# ─────────── Table Extraction ───────────\n",
    "def extract_tables(docx_path):\n",
    "    \"\"\"Extract tables from a Word document.\"\"\"\n",
    "    try:\n",
    "        # Load document\n",
    "        doc = Document(docx_path)\n",
    "        body = doc._element.body\n",
    "        tables = []\n",
    "        recent_paragraphs = []\n",
    "        paragraphs_between = []\n",
    "        table_index = 1\n",
    "        previous_table = None\n",
    "\n",
    "        for element in body:\n",
    "            # Handle paragraphs\n",
    "            if element.tag.endswith('}p'):\n",
    "                text = Paragraph(element, doc).text.strip()\n",
    "                if text:\n",
    "                    recent_paragraphs.append(text)\n",
    "                    paragraphs_between.append(text)\n",
    "                continue\n",
    "\n",
    "            # Handle tables\n",
    "            if element.tag.endswith('}tbl'):\n",
    "                table = Table(element, doc)\n",
    "\n",
    "                # Find table number and title\n",
    "                number = f\"TABLE {table_index}\"\n",
    "                title = f\"TABLE {table_index}\"\n",
    "                found_title = False\n",
    "\n",
    "                # Search recent paragraphs for table title\n",
    "                for i in range(len(recent_paragraphs) - 1, -1, -1):\n",
    "                    paragraph = recent_paragraphs[i]\n",
    "                    match = TABLE_TITLE_REGEX.match(paragraph)\n",
    "                    if match:\n",
    "                        number = paragraph\n",
    "                        title = recent_paragraphs[i + 1] if i + 1 < len(recent_paragraphs) else paragraph\n",
    "                        found_title = True\n",
    "                        break\n",
    "\n",
    "                # Extract table rows\n",
    "                rows = []\n",
    "                for row in table.rows:\n",
    "                    raw_cells = [get_cell_text(cell) for cell in row.cells]\n",
    "                    rows += expand_row(raw_cells)\n",
    "\n",
    "                # Normalize column count\n",
    "                max_columns = max(len(row) for row in rows)\n",
    "                for row in rows:\n",
    "                    while len(row) < max_columns:\n",
    "                        row.append(\"\")\n",
    "\n",
    "                # Check if table is a continuation\n",
    "                current_table = {\"table_content\": rows}\n",
    "                if previous_table and is_table_continuation(previous_table, current_table, paragraphs_between):\n",
    "                    previous_table[\"table_content\"].extend(rows[1:])  # Skip header row\n",
    "                    previous_table[\"needs_review\"] |= any(\n",
    "                        DOTS_SEPARATOR.search(cell) or cell == \"\" for row in rows for cell in row\n",
    "                    )\n",
    "                else:\n",
    "                    # Create new table entry\n",
    "                    table_entry = {\n",
    "                        \"table_number\": number,\n",
    "                        \"table_title\": title,\n",
    "                        \"table_content\": rows,\n",
    "                        \"needs_review\": any(\n",
    "                            DOTS_SEPARATOR.search(cell) or cell == \"\" for row in rows for cell in row\n",
    "                        )\n",
    "                    }\n",
    "                    tables.append(table_entry)\n",
    "                    previous_table = table_entry\n",
    "                    table_index += 1\n",
    "\n",
    "                # Reset paragraphs between tables\n",
    "                paragraphs_between.clear()\n",
    "                # Keep only the last 5 paragraphs\n",
    "                recent_paragraphs = recent_paragraphs[-5:] if len(recent_paragraphs) > 5 else recent_paragraphs\n",
    "\n",
    "        return tables\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {e}\")\n",
    "        return []\n",
    "\n",
    "# ─────────── Main Execution ───────────\n",
    "def main():\n",
    "    \"\"\"Main function: extract tables and save to JSON.\"\"\"\n",
    "    # Check if input file exists\n",
    "    if not os.path.exists(SOURCE_DOCX):\n",
    "        print(f\"Error: Input file {SOURCE_DOCX} not found.\")\n",
    "        return\n",
    "\n",
    "    # Extract tables\n",
    "    tables = extract_tables(SOURCE_DOCX)\n",
    "    if not tables:\n",
    "        print(\"No tables extracted.\")\n",
    "        return\n",
    "\n",
    "    # Save to JSON\n",
    "    try:\n",
    "        with open(DESTINATION_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(tables, f, indent=4, ensure_ascii=False)\n",
    "        review_count = sum(table[\"needs_review\"] for table in tables)\n",
    "        print(f\"🚩 {review_count} out of {len(tables)} tables need manual review → {DESTINATION_JSON}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic Detection and Extraction of Tables in PDF Documents via Visual Structure and OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script visually detects and extracts table images from a PDF document to be incorporated into the graphical interface, enabling easier formatting and handling of complex table structures.\n",
    "# Need install poppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# ─────────── Configuration ───────────\n",
    "# Tesseract and Poppler paths\n",
    "PYTESSERACT_PATH = r\"C:\\Users\\DEC_User\\Desktop\\tesserate\\tesseract.exe\"\n",
    "TESSDATA_PREFIX = r\"C:\\Users\\DEC_User\\Desktop\\tesserate\\tessdata\"\n",
    "POPPLER_PATH = r\"C:\\Users\\DEC_User\\Desktop\\poppler-23.11.0\\Library\\bin\"\n",
    "\n",
    "# Input PDF and output directory\n",
    "PDF_PATH = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\Documento Nativo.pdf\"\n",
    "OUTPUT_DIR = \"visual_tables\"\n",
    "PDF_DPI = 600\n",
    "\n",
    "# Set Tesseract configuration\n",
    "pytesseract.pytesseract.tesseract_cmd = PYTESSERACT_PATH\n",
    "os.environ[\"TESSDATA_PREFIX\"] = TESSDATA_PREFIX\n",
    "\n",
    "# ─────────── Helper Functions ───────────\n",
    "def preprocess_image_for_ocr(image):\n",
    "    \"\"\"Preprocess image for OCR by converting to grayscale and applying thresholding.\"\"\"\n",
    "    gray = np.array(image.convert(\"L\"))\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return Image.fromarray(binary)\n",
    "\n",
    "def roman_to_int(roman: str) -> int:\n",
    "    \"\"\"Convert Roman numerals to integer.\"\"\"\n",
    "    roman_values = {'I': 1, 'V': 5, 'X': 10, 'L': 50}\n",
    "    result = 0\n",
    "    prev_value = 0\n",
    "    try:\n",
    "        for char in reversed(roman):\n",
    "            value = roman_values[char]\n",
    "            if value >= prev_value:\n",
    "                result += value\n",
    "            else:\n",
    "                result -= value\n",
    "            prev_value = value\n",
    "        return result\n",
    "    except KeyError:\n",
    "        print(f\"Invalid Roman numeral: {roman}\")\n",
    "        return 0\n",
    "\n",
    "# ─────────── Table Extraction ───────────\n",
    "def extract_tables_visually(pdf_path: str, poppler_path: str, output_folder: str = OUTPUT_DIR, dpi: int = PDF_DPI):\n",
    "    \"\"\"Extract tables from PDF using computer vision and save as images.\"\"\"\n",
    "    # Check if input PDF exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: PDF file {pdf_path} not found.\")\n",
    "        return\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Convert PDF to images\n",
    "        pages = convert_from_path(pdf_path, dpi=dpi, poppler_path=poppler_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting PDF to images: {e}\")\n",
    "        return\n",
    "\n",
    "    table_counter = defaultdict(int)\n",
    "\n",
    "    for page_idx, page in enumerate(pages, start=1):\n",
    "        try:\n",
    "            # Convert page to grayscale and binarize\n",
    "            gray = np.array(page.convert(\"L\"))\n",
    "            _, binary = cv2.threshold(gray, 180, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "            # Detect horizontal and vertical lines\n",
    "            kernel_h = cv2.getStructuringElement(cv2.MORPH_RECT, (50, 1))\n",
    "            kernel_v = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 50))\n",
    "            lines = cv2.add(\n",
    "                cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_h),\n",
    "                cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel_v)\n",
    "            )\n",
    "\n",
    "            # Find contours (potential tables)\n",
    "            contours, _ = cv2.findContours(lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            contours = sorted(contours, key=lambda c: cv2.boundingRect(c)[1])  # Sort top-down\n",
    "\n",
    "            for contour in contours:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                # Filter small contours\n",
    "                if w < 100 or h < 40:\n",
    "                    continue\n",
    "\n",
    "                # Expand bounding box for context\n",
    "                x1 = max(0, x - 10)\n",
    "                x2 = min(page.width, x + w + 10)\n",
    "                y1 = max(0, y - 250)  # Include area above for title\n",
    "                y2 = y + h\n",
    "\n",
    "                # Crop and save table image\n",
    "                cropped = page.crop((x1, y1, x2, y2))\n",
    "                table_counter[page_idx] += 1\n",
    "                filename = f\"PAGE_{page_idx:03d}_table_{table_counter[page_idx]}.png\"\n",
    "                output_path = os.path.join(output_folder, filename)\n",
    "                cropped.save(output_path)\n",
    "                print(f\"🖼️ Saved: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing page {page_idx}: {e}\")\n",
    "\n",
    "# ─────────── Table Identification and Renaming ───────────\n",
    "def identify_and_rename_tables(folder: str):\n",
    "    \"\"\"Identify table titles using OCR and rename images.\"\"\"\n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"Error: Output folder {folder} not found.\")\n",
    "        return\n",
    "\n",
    "    # Get PNG files\n",
    "    files = sorted([f for f in os.listdir(folder) if f.endswith(\".png\")])\n",
    "    if not files:\n",
    "        print(f\"No PNG files found in {folder}.\")\n",
    "        return\n",
    "\n",
    "    used_names = defaultdict(int)\n",
    "    last_table_id = None\n",
    "\n",
    "    for file in files:\n",
    "        path = os.path.join(folder, file)\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(path)\n",
    "            width, height = image.size\n",
    "\n",
    "            # Crop top half for title detection\n",
    "            top_half = image.crop((0, 0, width, int(height * 0.5)))\n",
    "            top_half = preprocess_image_for_ocr(top_half)\n",
    "\n",
    "            # Perform OCR\n",
    "            text = pytesseract.image_to_string(top_half, lang=\"eng\", config=\"--psm 6\")\n",
    "            table_id = None\n",
    "\n",
    "            # Search for table title\n",
    "            for line in text.split(\"\\n\"):\n",
    "                line = line.strip().upper()\n",
    "                if line.startswith(\"QUADRO\"):\n",
    "                    match = re.search(r\"QUADRO\\s+([IVXLCDM]+)\", line)\n",
    "                    if match:\n",
    "                        table_id = f\"QUADRO_{match.group(1)}\"\n",
    "                    else:\n",
    "                        table_id = line.replace(\" \", \"_\")\n",
    "                    break\n",
    "\n",
    "            # Use last table ID if none found\n",
    "            if table_id:\n",
    "                last_table_id = table_id\n",
    "            elif last_table_id:\n",
    "                table_id = last_table_id\n",
    "            else:\n",
    "                print(f\"❌ Table not identified in: {file} — no previous table to fallback\")\n",
    "                continue\n",
    "\n",
    "            # Generate new filename\n",
    "            used_names[table_id] += 1\n",
    "            new_name = f\"{table_id}_{used_names[table_id]}.png\"\n",
    "            new_path = os.path.join(folder, new_name)\n",
    "\n",
    "            # Rename file if new path doesn't exist\n",
    "            if not os.path.exists(new_path):\n",
    "                os.rename(path, new_path)\n",
    "                print(f\"✅ Renamed: {file} → {new_name}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Already exists: {new_name} — skipped to avoid overwrite\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# ─────────── Main Execution ───────────\n",
    "def main():\n",
    "    \"\"\"Main function: extract and rename table images from PDF.\"\"\"\n",
    "    # Check Tesseract and Poppler paths\n",
    "    if not os.path.exists(PYTESSERACT_PATH):\n",
    "        print(f\"Error: Tesseract executable not found at {PYTESSERACT_PATH}.\")\n",
    "        return\n",
    "    if not os.path.exists(POPPLER_PATH):\n",
    "        print(f\"Error: Poppler not found at {POPPLER_PATH}.\")\n",
    "        return\n",
    "\n",
    "    # Extract tables\n",
    "    extract_tables_visually(PDF_PATH, POPPLER_PATH)\n",
    "    # Identify and rename tables\n",
    "    identify_and_rename_tables(OUTPUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive Table Editor for Visual Review and JSON Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script provides a graphical interface to review, edit, and organize tables extracted from documents. It allows users to view table content and related images, edit titles and data, add or remove rows, merge or delete tables, and export the updated data to a new JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import glob\n",
    "\n",
    "# ─────────── Configuration ───────────\n",
    "# Input and output JSON files and image directory\n",
    "JSON_INPUT = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\extracted_tables\\Tables_clean.json\"\n",
    "JSON_OUTPUT = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\extracted_tables\\Tables_clean_final.json\"\n",
    "IMAGE_DIR = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\visual_tables\"\n",
    "\n",
    "# ─────────── Utility Functions ───────────\n",
    "def load_tables(path: str):\n",
    "    \"\"\"Load table data from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON {path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_tables(path: str, tables):\n",
    "    \"\"\"Save table data to a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(tables, f, indent=4, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON {path}: {e}\")\n",
    "\n",
    "def build_table_summary(table_number, table_title, content):\n",
    "    \"\"\"Generate a natural-language summary of a table.\"\"\"\n",
    "    if not content or len(content) < 2:\n",
    "        return \"\"\n",
    "    header = content[0]\n",
    "    rows = content[1:]\n",
    "    summary = f\"{table_number} presents '{table_title}' categorized by '{header[0]}'. \"\n",
    "    for row in rows:\n",
    "        parts = [f\"'{val}' '{header[i]}'\" for i, val in enumerate(row)]\n",
    "        phrase = f\"For {parts[0]}, there should be {parts[1]}\" + (f\" with {parts[2]}\" if len(parts) > 2 else \"\") + \". \"\n",
    "        summary += phrase\n",
    "    return summary.strip()\n",
    "\n",
    "def attach_summaries(tables):\n",
    "    \"\"\"Add or update the 'table_summary' field for each table.\"\"\"\n",
    "    for table in tables:\n",
    "        table[\"table_summary\"] = build_table_summary(\n",
    "            table.get(\"table_number\", \"N/A\"),\n",
    "            table.get(\"table_title\", \"Untitled Table\"),\n",
    "            table.get(\"table_content\", [])\n",
    "        )\n",
    "\n",
    "# ─────────── GUI Class ───────────\n",
    "class TableReviewer(tk.Tk):\n",
    "    \"\"\"GUI for reviewing, editing, merging, and deleting extracted tables.\"\"\"\n",
    "    IMG_MAX_WIDTH, IMG_MAX_HEIGHT = 500, 380\n",
    "\n",
    "    def __init__(self, tables):\n",
    "        super().__init__()\n",
    "        self.title(\"Table Reviewer\")\n",
    "        self.state(\"zoomed\")\n",
    "\n",
    "        # Configure ttk style\n",
    "        style = ttk.Style(self)\n",
    "        style.theme_use(\"clam\")\n",
    "        style.configure(\"Treeview.Heading\", font=(\"Segoe UI\", 10, \"bold\"))\n",
    "        style.configure(\"Treeview\", rowheight=26, font=(\"Segoe UI\", 10))\n",
    "\n",
    "        # Initialize data\n",
    "        self.tables = tables\n",
    "        self.current = None\n",
    "        self.img_labels = []\n",
    "        self.img_caches = []\n",
    "        self.original_listbox_items = []\n",
    "\n",
    "        # Layout: Split window into left and right panes\n",
    "        paned = ttk.Panedwindow(self, orient=tk.HORIZONTAL)\n",
    "        paned.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Left panel: Table list\n",
    "        left_frame = ttk.Frame(paned, padding=5)\n",
    "        ttk.Label(left_frame, text=\"Tables\", font=(\"Segoe UI\", 11, \"bold\")).pack()\n",
    "        self.listbox = tk.Listbox(left_frame, width=38, activestyle=\"none\", selectmode=tk.BROWSE)\n",
    "        self.listbox.pack(fill=tk.BOTH, expand=True, pady=5)\n",
    "        scrollbar = ttk.Scrollbar(left_frame, command=self.listbox.yview)\n",
    "        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        self.listbox.configure(yscroll=scrollbar.set)\n",
    "\n",
    "        # Populate listbox\n",
    "        for table in tables:\n",
    "            icon = \"⚠️\" if table.get(\"needs_review\", False) else \"✔️\"\n",
    "            item_text = f\"{icon}  {table['table_number']} — {table['table_title']}\"\n",
    "            self.original_listbox_items.append(item_text)\n",
    "            self.listbox.insert(tk.END, item_text)\n",
    "\n",
    "        self.listbox.bind(\"<<ListboxSelect>>\", self.show_table)\n",
    "        self.listbox.bind(\"<Configure>\", self.update_listbox_items)\n",
    "\n",
    "        # Right panel: Table details and editor\n",
    "        right_frame = ttk.Frame(paned, padding=5)\n",
    "\n",
    "        # Table info (number and title)\n",
    "        info_frame = ttk.Frame(right_frame)\n",
    "        info_frame.pack(fill=tk.X, pady=(0, 6))\n",
    "        info_frame.columnconfigure(1, weight=1)\n",
    "        info_frame.columnconfigure(3, weight=3)\n",
    "        ttk.Label(info_frame, text=\"Number:\").grid(row=0, column=0, sticky=\"e\", padx=(0, 4))\n",
    "        self.num_var = tk.StringVar()\n",
    "        ttk.Entry(info_frame, textvariable=self.num_var).grid(row=0, column=1, sticky=\"ew\", padx=(0, 8))\n",
    "        ttk.Label(info_frame, text=\"Title:\").grid(row=0, column=2, sticky=\"e\", padx=(0, 8))\n",
    "        self.title_var = tk.StringVar()\n",
    "        self.title_entry = ttk.Entry(info_frame, textvariable=self.title_var)\n",
    "        self.title_entry.grid(row=0, column=3, sticky=\"ew\", padx=(0, 8))\n",
    "        ttk.Button(info_frame, text=\"💾 Update\", command=self.update_info).grid(row=0, column=4, padx=2)\n",
    "        self.title_entry.bind(\"<Configure>\", self.update_title_display)\n",
    "\n",
    "        # Image display\n",
    "        self.img_frame = ttk.Frame(right_frame)\n",
    "        self.img_frame.pack(fill=tk.X, pady=(0, 6))\n",
    "\n",
    "        # Table content grid (Treeview)\n",
    "        self.tree = ttk.Treeview(right_frame, show=\"headings\")\n",
    "        y_scrollbar = ttk.Scrollbar(right_frame, command=self.tree.yview)\n",
    "        y_scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        x_scrollbar = ttk.Scrollbar(right_frame, command=self.tree.xview, orient=tk.HORIZONTAL)\n",
    "        x_scrollbar.pack(side=tk.BOTTOM, fill=tk.X)\n",
    "        self.tree.configure(yscroll=y_scrollbar.set, xscroll=x_scrollbar.set)\n",
    "        self.tree.pack(fill=tk.BOTH, expand=True)\n",
    "        self.tree.bind(\"<Double-1>\", self.start_edit)\n",
    "\n",
    "        # Action buttons\n",
    "        button_frame = ttk.Frame(right_frame)\n",
    "        button_frame.pack(fill=tk.X, pady=4)\n",
    "        ttk.Button(button_frame, text=\"➕ Add Row\", command=self.add_row).pack(side=tk.LEFT, padx=4)\n",
    "        ttk.Button(button_frame, text=\"➖ Remove Row\", command=self.remove_row).pack(side=tk.LEFT, padx=4)\n",
    "        ttk.Button(button_frame, text=\"📄 Duplicate Row\", command=self.duplicate_row).pack(side=tk.LEFT, padx=4)\n",
    "        ttk.Button(button_frame, text=\"✔️ Mark Reviewed\", command=self.mark_reviewed).pack(side=tk.LEFT, padx=8)\n",
    "        ttk.Button(button_frame, text=\"🗑️ Delete\", command=self.delete_tables_window).pack(side=tk.RIGHT, padx=4)\n",
    "        ttk.Button(button_frame, text=\"💾 Save (Ctrl+S)\", command=self.save_json).pack(side=tk.RIGHT, padx=4)\n",
    "        ttk.Button(button_frame, text=\"🔀 Merge\", command=self.merge_tables_window).pack(side=tk.RIGHT, padx=4)\n",
    "\n",
    "        # Add panes to paned window\n",
    "        paned.add(left_frame, weight=1)\n",
    "        paned.add(right_frame, weight=4)\n",
    "\n",
    "        # Global key binding\n",
    "        self.bind(\"<Control-s>\", lambda *_: self.save_json())\n",
    "\n",
    "    # ─────────── Helper Methods ───────────\n",
    "    def update_listbox_items(self, *_):\n",
    "        \"\"\"Update listbox items based on available width.\"\"\"\n",
    "        width = self.listbox.winfo_width()\n",
    "        visible_chars = max(1, width // 7)\n",
    "        selected = self.listbox.curselection()\n",
    "        self.listbox.delete(0, tk.END)\n",
    "        for text in self.original_listbox_items:\n",
    "            self.listbox.insert(\n",
    "                tk.END,\n",
    "                text if len(text) <= visible_chars else text[:visible_chars - 3] + \"...\"\n",
    "            )\n",
    "        if selected:\n",
    "            self.listbox.selection_set(selected)\n",
    "            self.listbox.see(selected)\n",
    "\n",
    "    def update_title_display(self, *_):\n",
    "        \"\"\"Update title entry display based on available width.\"\"\"\n",
    "        width = self.title_entry.winfo_width()\n",
    "        visible_chars = max(1, width // 7)\n",
    "        full_text = self.title_var.get()\n",
    "        self.title_entry.delete(0, tk.END)\n",
    "        self.title_entry.insert(\n",
    "            0,\n",
    "            full_text if len(full_text) <= visible_chars else full_text[:visible_chars - 3] + \"...\"\n",
    "        )\n",
    "\n",
    "    def show_table(self, *_):\n",
    "        \"\"\"Display selected table details, images, and content.\"\"\"\n",
    "        selection = self.listbox.curselection()\n",
    "        if not selection:\n",
    "            return\n",
    "        self.current = selection[0]\n",
    "        table = self.tables[self.current]\n",
    "        data = table[\"table_content\"]\n",
    "\n",
    "        # Update info\n",
    "        self.num_var.set(table[\"table_number\"])\n",
    "        self.title_var.set(table[\"table_title\"])\n",
    "        self.update_title_display()\n",
    "\n",
    "        # Clear and load images\n",
    "        for label in self.img_labels:\n",
    "            label.destroy()\n",
    "        self.img_labels.clear()\n",
    "        self.img_caches.clear()\n",
    "\n",
    "        table_number = table[\"table_number\"].replace(\" \", \"_\")\n",
    "        img_pattern = os.path.join(IMAGE_DIR, f\"{table_number}_*.png\")\n",
    "        img_paths = sorted(glob.glob(img_pattern))\n",
    "\n",
    "        if img_paths:\n",
    "            for img_path in img_paths:\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                    img.thumbnail((self.IMG_MAX_WIDTH, self.IMG_MAX_HEIGHT))\n",
    "                    photo = ImageTk.PhotoImage(img)\n",
    "                    label = ttk.Label(self.img_frame, image=photo)\n",
    "                    label.pack(pady=2)\n",
    "                    self.img_labels.append(label)\n",
    "                    self.img_caches.append(photo)  # Prevent garbage collection\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {img_path}: {e}\")\n",
    "        else:\n",
    "            ttk.Label(self.img_frame, text=\"(No image available)\").pack(pady=2)\n",
    "\n",
    "        # Update Treeview\n",
    "        self.tree.delete(*self.tree.get_children())\n",
    "        self.tree[\"columns\"] = list(range(len(data[0])))\n",
    "        for i, col in enumerate(self.tree[\"columns\"]):\n",
    "            self.tree.heading(col, text=data[0][i] or f\"Column {i+1}\")\n",
    "            self.tree.column(col, width=160, stretch=True)\n",
    "        for row in data[1:]:\n",
    "            self.tree.insert(\"\", tk.END, values=row)\n",
    "\n",
    "    # ─────────── Table Editing ───────────\n",
    "    def start_edit(self, event):\n",
    "        \"\"\"Enable in-place editing of a Treeview cell.\"\"\"\n",
    "        if self.tree.identify(\"region\", event.x, event.y) != \"cell\":\n",
    "            return\n",
    "        row_id = self.tree.identify_row(event.y)\n",
    "        col_idx = int(self.tree.identify_column(event.x)[1:]) - 1\n",
    "        x, y, w, h = self.tree.bbox(row_id, f\"#{col_idx + 1}\")\n",
    "        entry = tk.Entry(self.tree)\n",
    "        entry.place(x=x, y=y, width=w, height=h)\n",
    "        entry.insert(0, self.tree.item(row_id)[\"values\"][col_idx])\n",
    "        entry.focus()\n",
    "\n",
    "        def commit(_=None):\n",
    "            values = list(self.tree.item(row_id)[\"values\"])\n",
    "            values[col_idx] = entry.get()\n",
    "            self.tree.item(row_id, values=values)\n",
    "            entry.destroy()\n",
    "\n",
    "        entry.bind(\"<Return>\", commit)\n",
    "        entry.bind(\"<FocusOut>\", commit)\n",
    "\n",
    "    def add_row(self):\n",
    "        \"\"\"Add a new empty row to the Treeview.\"\"\"\n",
    "        self.tree.insert(\"\", tk.END, values=[\"\"] * len(self.tree[\"columns\"]))\n",
    "\n",
    "    def duplicate_row(self):\n",
    "        \"\"\"Duplicate selected rows in the Treeview.\"\"\"\n",
    "        for row in self.tree.selection():\n",
    "            self.tree.insert(\"\", tk.END, values=self.tree.item(row)[\"values\"])\n",
    "\n",
    "    def remove_row(self):\n",
    "        \"\"\"Remove selected rows from the Treeview.\"\"\"\n",
    "        for row in self.tree.selection():\n",
    "            self.tree.delete(row)\n",
    "\n",
    "    # ─────────── Table Metadata ───────────\n",
    "    def update_info(self):\n",
    "        \"\"\"Update table number and title.\"\"\"\n",
    "        if self.current is None:\n",
    "            return\n",
    "        table = self.tables[self.current]\n",
    "        table[\"table_number\"] = self.num_var.get()\n",
    "        table[\"table_title\"] = self.title_var.get()\n",
    "\n",
    "        # Update listbox\n",
    "        icon = \"⚠️\" if table.get(\"needs_review\", False) else \"✔️\"\n",
    "        item_text = f\"{icon}  {table['table_number']} — {table['table_title']}\"\n",
    "        self.original_listbox_items[self.current] = item_text\n",
    "        self.listbox.delete(self.current)\n",
    "        self.listbox.insert(self.current, item_text)\n",
    "        self.update_listbox_items()\n",
    "        self.update_title_display()\n",
    "\n",
    "    def mark_reviewed(self):\n",
    "        \"\"\"Mark the current table as reviewed.\"\"\"\n",
    "        if self.current is None:\n",
    "            return\n",
    "        self.tables[self.current][\"needs_review\"] = False\n",
    "        item_text = self.listbox.get(self.current).replace(\"⚠️\", \"✔️\")\n",
    "        self.original_listbox_items[self.current] = item_text\n",
    "        self.listbox.delete(self.current)\n",
    "        self.listbox.insert(self.current, item_text)\n",
    "        self.listbox.itemconfig(self.current, foreground=\"green\")\n",
    "        self.update_listbox_items()\n",
    "\n",
    "    # ─────────── Save and Export ───────────\n",
    "    def save_json(self, *_):\n",
    "        \"\"\"Save updated tables to JSON.\"\"\"\n",
    "        if self.current is not None:\n",
    "            rows = [self.tree.item(i)[\"values\"] for i in self.tree.get_children()]\n",
    "            headers = [self.tree.heading(c)[\"text\"] for c in self.tree[\"columns\"]]\n",
    "            table = self.tables[self.current]\n",
    "            table[\"table_content\"] = [headers] + rows\n",
    "            table[\"table_number\"] = self.num_var.get()\n",
    "            table[\"table_title\"] = self.title_var.get()\n",
    "\n",
    "        attach_summaries(self.tables)  # Update summaries before saving\n",
    "        save_tables(JSON_OUTPUT, self.tables)\n",
    "        messagebox.showinfo(\"Saved\", f\"Saved to:\\n{JSON_OUTPUT}\")\n",
    "\n",
    "    # ─────────── Merge Tables ───────────\n",
    "    def merge_tables_window(self):\n",
    "        \"\"\"Open a window to select tables for merging.\"\"\"\n",
    "        window = tk.Toplevel(self)\n",
    "        window.title(\"Merge Tables\")\n",
    "        ttk.Label(window, text=\"Select tables to merge:\", font=(\"Segoe UI\", 11, \"bold\")).pack(pady=5)\n",
    "\n",
    "        self.merge_vars = []\n",
    "        for idx, table in enumerate(self.tables):\n",
    "            var = tk.BooleanVar(value=False)\n",
    "            chk = ttk.Checkbutton(window, text=f\"{table['table_number']} — {table['table_title'][:50]}\", variable=var)\n",
    "            chk.pack(anchor=\"w\", padx=10)\n",
    "            self.merge_vars.append((var, idx))\n",
    "\n",
    "        ttk.Label(window, text=\"Number of the new table:\").pack(pady=(8, 0))\n",
    "        num_entry = ttk.Entry(window, width=20)\n",
    "        num_entry.pack(pady=4)\n",
    "        ttk.Label(window, text=\"Title of the new table:\").pack()\n",
    "        title_entry = ttk.Entry(window, width=60)\n",
    "        title_entry.pack(pady=4)\n",
    "        ttk.Button(\n",
    "            window,\n",
    "            text=\"✅ Merge\",\n",
    "            command=lambda: self.merge_selected_tables(window, num_entry.get(), title_entry.get())\n",
    "        ).pack(pady=10)\n",
    "\n",
    "    def merge_selected_tables(self, window, new_number, new_title):\n",
    "        \"\"\"Merge selected tables into a new table.\"\"\"\n",
    "        selected_idxs = [idx for var, idx in self.merge_vars if var.get()]\n",
    "        if len(selected_idxs) < 2:\n",
    "            messagebox.showwarning(\"Invalid selection\", \"Select at least two tables.\")\n",
    "            return\n",
    "\n",
    "        # Check column compatibility\n",
    "        num_cols = len(self.tables[selected_idxs[0]][\"table_content\"][0])\n",
    "        for idx in selected_idxs:\n",
    "            if len(self.tables[idx][\"table_content\"][0]) != num_cols:\n",
    "                messagebox.showerror(\"Error\", \"Selected tables must have the same number of columns!\")\n",
    "                return\n",
    "\n",
    "        # Merge content\n",
    "        merged_content = [self.tables[selected_idxs[0]][\"table_content\"][0]]  # Header\n",
    "        for idx in selected_idxs:\n",
    "            merged_content.extend(self.tables[idx][\"table_content\"][1:])  # Rows\n",
    "\n",
    "        # Create new table\n",
    "        merged_table = {\n",
    "            \"table_number\": new_number or \"N/A\",\n",
    "            \"table_title\": new_title or \"Merged Table\",\n",
    "            \"table_content\": merged_content,\n",
    "            \"needs_review\": True,\n",
    "            \"image_path\": None\n",
    "        }\n",
    "\n",
    "        # Add to tables and listbox\n",
    "        self.tables.append(merged_table)\n",
    "        icon = \"⚠️\"\n",
    "        item_text = f\"{icon}  {merged_table['table_number']} — {merged_table['table_title']}\"\n",
    "        self.original_listbox_items.append(item_text)\n",
    "        self.listbox.insert(tk.END, item_text)\n",
    "        messagebox.showinfo(\"Tables merged\", \"The new merged table was added successfully!\")\n",
    "        self.update_listbox_items()\n",
    "        window.destroy()\n",
    "\n",
    "    # ─────────── Delete Tables ───────────\n",
    "    def delete_tables_window(self):\n",
    "        \"\"\"Open a window to select tables for deletion.\"\"\"\n",
    "        window = tk.Toplevel(self)\n",
    "        window.title(\"Delete Tables\")\n",
    "        ttk.Label(window, text=\"Select tables to delete:\", font=(\"Segoe UI\", 11, \"bold\")).pack(pady=5)\n",
    "\n",
    "        self.delete_vars = []\n",
    "        for idx, table in enumerate(self.tables):\n",
    "            var = tk.BooleanVar(value=False)\n",
    "            chk = ttk.Checkbutton(window, text=f\"{table['table_number']} — {table['table_title'][:50]}\", variable=var)\n",
    "            chk.pack(anchor=\"w\", padx=10)\n",
    "            self.delete_vars.append((var, idx))\n",
    "\n",
    "        ttk.Button(window, text=\"🗑️ Delete\", command=lambda: self.delete_selected_tables(window)).pack(pady=10)\n",
    "\n",
    "    def delete_selected_tables(self, window):\n",
    "        \"\"\"Delete selected tables.\"\"\"\n",
    "        selected_idxs = [idx for var, idx in self.delete_vars if var.get()]\n",
    "        if not selected_idxs:\n",
    "            messagebox.showwarning(\"Invalid selection\", \"Select at least one table to delete.\")\n",
    "            return\n",
    "\n",
    "        # Delete in reverse order to preserve indices\n",
    "        selected_idxs.sort(reverse=True)\n",
    "        for idx in selected_idxs:\n",
    "            self.tables.pop(idx)\n",
    "            self.original_listbox_items.pop(idx)\n",
    "            self.listbox.delete(idx)\n",
    "\n",
    "        # Clear right panel if current table was deleted\n",
    "        if self.current is not None and self.current in selected_idxs:\n",
    "            self.current = None\n",
    "            self.num_var.set(\"\")\n",
    "            self.title_var.set(\"\")\n",
    "            for label in self.img_labels:\n",
    "                label.destroy()\n",
    "            self.img_labels.clear()\n",
    "            self.img_caches.clear()\n",
    "            self.tree.delete(*self.tree.get_children())\n",
    "            self.tree[\"columns\"] = []\n",
    "\n",
    "        self.update_listbox_items()\n",
    "        messagebox.showinfo(\"Tables deleted\", \"Selected tables were removed successfully!\")\n",
    "        window.destroy()\n",
    "\n",
    "# ─────────── Main Execution ───────────\n",
    "def main():\n",
    "    \"\"\"Main function: load tables and launch GUI.\"\"\"\n",
    "    # Check if input JSON exists\n",
    "    if not os.path.exists(JSON_INPUT):\n",
    "        messagebox.showerror(\"Error\", f\"Input JSON file not found: {JSON_INPUT}\")\n",
    "        return\n",
    "\n",
    "    # Check if image directory exists\n",
    "    if not os.path.exists(IMAGE_DIR):\n",
    "        messagebox.showerror(\"Error\", f\"Image directory not found: {IMAGE_DIR}\")\n",
    "        return\n",
    "\n",
    "    # Load tables and start GUI\n",
    "    tables = load_tables(JSON_INPUT)\n",
    "    if not tables:\n",
    "        messagebox.showerror(\"Error\", \"No tables loaded from JSON.\")\n",
    "        return\n",
    "\n",
    "    TableReviewer(tables).mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated Table Summarization and Translation with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "# ─────────── OpenAI Configuration ───────────\n",
    "# Initialize OpenAI client\n",
    "CLIENT = OpenAI(\n",
    "    api_key=\"ddc-temp-free-e3b73cd814cc4f3ea79b5d4437912663\",\n",
    "    base_url=\"https://api.devsdocode.com/v1\",\n",
    ")\n",
    "\n",
    "# ─────────── File Paths ───────────\n",
    "# Base directory and JSON file paths\n",
    "BASE_DIR = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\"\n",
    "INPUT_JSON = os.path.join(BASE_DIR, \"extracted_tables\", \"Tables_clean_final.json\")\n",
    "OUTPUT_JSON = os.path.join(BASE_DIR, \"extracted_tables\", \"Tables_clean_final_with_summary.json\")\n",
    "METADATA_JSON = os.path.join(BASE_DIR, \"Documento_Nativo_results_gui.json\")\n",
    "\n",
    "# ─────────── Utility Functions ───────────\n",
    "def load_document_language(path: str) -> str:\n",
    "    \"\"\"Load document language from metadata JSON.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, encoding=\"utf-8\") as f:\n",
    "                return json.load(f).get(\"language\", \"en\").lower()\n",
    "        return \"en\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata {path}: {e}\")\n",
    "        return \"en\"\n",
    "\n",
    "# Load document language\n",
    "DOCUMENT_LANGUAGE = load_document_language(METADATA_JSON)\n",
    "\n",
    "def query_openai(messages, temperature=0.2, max_tokens=800):\n",
    "    \"\"\"Query OpenAI API and return response.\"\"\"\n",
    "    try:\n",
    "        response = CLIENT.chat.completions.create(\n",
    "            model=\"provider-4/gpt-4.1\",\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ OpenAI API error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def fallback_summary_english(table):\n",
    "    \"\"\"Generate a fallback English summary for a table.\"\"\"\n",
    "    content = table.get(\"table_content\", [])\n",
    "    if len(content) < 2 or not isinstance(content[0], list):\n",
    "        return \"\"\n",
    "    header, rows = content[0], content[1:]\n",
    "    summary = [f\"Table {table.get('table_number', 'N/A')}, titled '{table.get('table_title', 'Untitled')}', lists:\"]\n",
    "    for row in rows:\n",
    "        summary.append(\", \".join(f\"'{row[i]}' for '{header[i]}'\" for i in range(min(len(row), len(header)))) + \".\")\n",
    "    return \" \".join(summary)\n",
    "\n",
    "# ─────────── Summary Generation ───────────\n",
    "def generate_native_summary(table, language):\n",
    "    \"\"\"Generate a detailed summary in the native language.\"\"\"\n",
    "    table_content = json.dumps(table[\"table_content\"], ensure_ascii=False)\n",
    "    prompt = (\n",
    "        f\"Write a detailed, single-paragraph summary of this table in {language.upper()}, \"\n",
    "        f\"listing all column headers and values using full sentences and single quotes. \"\n",
    "        f\"No bullet points.\\n\\nTable:\\n{table_content}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"You summarize tables into technical prose in {language.upper()}.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    return query_openai(messages)\n",
    "\n",
    "def translate_to_english(text):\n",
    "    \"\"\"Translate a summary to English, preserving numbers and quoted values.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    prompt = (\n",
    "        f\"Translate the following technical paragraph into English. \"\n",
    "        f\"Keep all numbers and quoted values exactly the same:\\n\\n{text}\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a technical translator to English.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    return query_openai(messages, temperature=0.0)\n",
    "\n",
    "# ─────────── Main Execution ───────────\n",
    "def generate_all_summaries():\n",
    "    \"\"\"Generate native and English summaries for all tables.\"\"\"\n",
    "    # Check if input JSON exists\n",
    "    if not os.path.exists(INPUT_JSON):\n",
    "        print(f\"Error: Input JSON file not found: {INPUT_JSON}\")\n",
    "        return\n",
    "\n",
    "    # Load tables\n",
    "    try:\n",
    "        with open(INPUT_JSON, encoding=\"utf-8\") as f:\n",
    "            tables = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON {INPUT_JSON}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Process each table\n",
    "    for idx, table in enumerate(tables, 1):\n",
    "        print(f\"[{idx}/{len(tables)}] Processing {table.get('table_number', 'N/A')}…\")\n",
    "\n",
    "        # Generate native summary\n",
    "        native_summary = generate_native_summary(table, DOCUMENT_LANGUAGE)\n",
    "        if not native_summary:\n",
    "            print(\"   ↪ Using fallback summary.\")\n",
    "            native_summary = fallback_summary_english(table)\n",
    "\n",
    "        # Translate to English\n",
    "        english_summary = translate_to_english(native_summary)\n",
    "\n",
    "        # Update table with summaries\n",
    "        table[\"table_summary\"] = native_summary\n",
    "        table[\"table_summary_en\"] = english_summary\n",
    "\n",
    "        # Rate limiting to avoid API overload\n",
    "        time.sleep(2.5)\n",
    "\n",
    "    # Save updated tables\n",
    "    try:\n",
    "        with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(tables, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\n✅ Summaries written to: {OUTPUT_JSON}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON {OUTPUT_JSON}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_all_summaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT-Based Multi-Class Text Classifier for Legal Document Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following the annotation process in Ango Hub, a BERT-based neural network is trained to perform hierarchical classification of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, TrainerCallback\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "# ─────────── Configuration ───────────\n",
    "# Path to annotated JSON data\n",
    "INPUT_JSON_PATH = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\FIREBIM_TEXT-task-export-2025-05-05-11_25_03_GMT.json\"\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "\n",
    "# ─────────── Data Loading ───────────\n",
    "def load_data(path: str):\n",
    "    \"\"\"Load and parse annotated JSON data.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"JSON file not found at {path}\")\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON {path}: {e}\")\n",
    "        return []\n",
    "    records = []\n",
    "    for item in data:\n",
    "        task = item.get(\"task\", {})\n",
    "        tools = task.get(\"tools\", [])\n",
    "        for tool in tools:\n",
    "            ner_data = tool.get(\"ner\", {})\n",
    "            text = ner_data.get(\"selection\", \"\").strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            label = tool.get(\"title\", \"Trash\")\n",
    "            records.append({\"text\": text, \"label\": label})\n",
    "    return records\n",
    "\n",
    "# ─────────── Data Preparation ───────────\n",
    "def prepare_dataframe(records):\n",
    "    \"\"\"Create DataFrame and map labels to IDs.\"\"\"\n",
    "    df = pd.DataFrame(records)\n",
    "    labels_map = {\n",
    "        \"Trash\": \"Trash\",\n",
    "        \"Chapter Number\": \"Chapter Number\",\n",
    "        \"Chapter Title\": \"Chapter Title\",\n",
    "        \"Article Number\": \"Article Number\",\n",
    "        \"Article Title\": \"Article Title\",\n",
    "        \"Item\": \"Item\",\n",
    "        \"Sub Item\": \"Sub Item\",\n",
    "        \"Sub Sub Item\": \"Sub Sub Item\",\n",
    "        \"Annex Number\": \"Annex Number\",\n",
    "        \"Annex Title\": \"Annex Title\",\n",
    "        \"Title Title\": \"Title Title\",\n",
    "        \"Ref Tables\": \"Ref Tables\",\n",
    "        \"Title Number\": \"Title Number\",\n",
    "        \"Section Number\": \"Section Number\",\n",
    "        \"Section Title\": \"Section Title\"\n",
    "    }\n",
    "    label_to_id = {label: i for i, label in enumerate(labels_map.keys())}\n",
    "    df[\"label_id\"] = df[\"label\"].map(label_to_id)\n",
    "    missing_labels = df[\"label_id\"].isna().sum()\n",
    "    if missing_labels > 0:\n",
    "        print(f\"Warning: {missing_labels} records have invalid labels. Assigning 'Trash'.\")\n",
    "        df[\"label_id\"] = df[\"label_id\"].fillna(label_to_id[\"Trash\"])\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(df[\"label\"].value_counts())\n",
    "    return df, label_to_id\n",
    "\n",
    "def compute_class_weights(df, label_to_id):\n",
    "    \"\"\"Calculate class weights for imbalanced data.\"\"\"\n",
    "    class_counts = df[\"label_id\"].value_counts().sort_index()\n",
    "    total_samples = len(df)\n",
    "    return torch.tensor(\n",
    "        [total_samples / (len(class_counts) * count) for count in class_counts],\n",
    "        dtype=torch.float\n",
    "    )\n",
    "\n",
    "# ─────────── Dataset Definition ───────────\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for text classification.\"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ─────────── Model and Trainer ───────────\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer with class weights for imbalanced classes.\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        loss = loss_fn(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute precision, recall, and F1 metrics.\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    prec_weighted, rec_weighted, f1_weighted, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(labels, preds, average=\"macro\")\n",
    "    return {\n",
    "        \"precision_weighted\": prec_weighted,\n",
    "        \"recall_weighted\": rec_weighted,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"precision_macro\": prec_macro,\n",
    "        \"recall_macro\": rec_macro,\n",
    "        \"f1_macro\": f1_macro\n",
    "    }\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    \"\"\"Callback for early stopping based on validation F1 score.\"\"\"\n",
    "    def __init__(self, patience=3):\n",
    "        self.patience = patience\n",
    "        self.best_f1 = None\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        f1 = metrics.get(\"eval_f1_macro\")\n",
    "        if f1 is None:\n",
    "            return\n",
    "        if self.best_f1 is None or f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            if self.patience_counter >= self.patience:\n",
    "                print(\"\\n⛔ Early stopping triggered!\")\n",
    "                control.should_training_stop = True\n",
    "\n",
    "# ─────────── Main Execution ───────────\n",
    "def main():\n",
    "    \"\"\"Main function: load data, train model, and save results.\"\"\"\n",
    "    # Disable Weights & Biases logging\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "    # Load and prepare data\n",
    "    records = load_data(INPUT_JSON_PATH)\n",
    "    if not records:\n",
    "        print(\"No valid records found.\")\n",
    "        return\n",
    "    df, label_to_id = prepare_dataframe(records)\n",
    "    class_weights = compute_class_weights(df, label_to_id)\n",
    "\n",
    "    # Split data\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        df[\"text\"].tolist(),\n",
    "        df[\"label_id\"].tolist(),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df[\"label_id\"]\n",
    "    )\n",
    "\n",
    "    # Initialize tokenizer and datasets\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "    # Initialize model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label_to_id))\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = True\n",
    "    model.config.attention_probs_dropout_prob = 0.2\n",
    "    model.config.hidden_dropout_prob = 0.2\n",
    "\n",
    "    # Configure training\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./roberta_text_classifier\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_steps=300,\n",
    "        weight_decay=0.2,\n",
    "        logging_steps=20,\n",
    "        report_to=\"none\",\n",
    "        seed=42,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        fp16=False,\n",
    "        gradient_accumulation_steps=1\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(patience=10)],\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\n🔴 Starting Training...\\n\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save model and tokenizer\n",
    "    try:\n",
    "        model.save_pretrained(\"./roberta_text_classifier\")\n",
    "        tokenizer.save_pretrained(\"./roberta_text_classifier\")\n",
    "        print(\"\\n✅ Training completed and model saved!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT Model Testing on Word Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During the finalization of the fine-tuning process for the BERT model used for legal text classification, it was necessary to reorganize the generated files. Specifically, some critical files that were outside the checkpoint folder were moved into the corresponding checkpoint-* subfolder. These files are:\n",
    "\n",
    "# pytorch_model.bin: Contains the trained model weights.\n",
    "# config.json: Defines the model architecture and parameters.\n",
    "# vocab.txt: Vocabulary used by the tokenizer.\n",
    "# tokenizer_config.json: Tokenizer configuration.\n",
    "# special_tokens_map.json: Mapping of special tokens (e.g., [CLS], [SEP])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GUI-Based Legal Text Classification and JSON Structuring with Fine-Tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from docx import Document\n",
    "import os\n",
    "\n",
    "# ─────────── Configuration ───────────\n",
    "# Paths for input document, output JSON, and trained model\n",
    "INPUT_DOCX_PATH = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\Documento_Nativo.docx\"\n",
    "OUTPUT_JSON_BASE = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\structured_output\"\n",
    "MODEL_PATH = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\bert_text_classifier_roberta\\checkpoint-646\"\n",
    "\n",
    "# Mapping of label IDs to human-readable labels\n",
    "ID_TO_LABEL = {\n",
    "    0: \"Trash\",\n",
    "    8: \"Annex Number\",\n",
    "    9: \"Annex Title\",\n",
    "    13: \"Title Number\",\n",
    "    10: \"Title Title\",\n",
    "    1: \"Chapter Number\",\n",
    "    2: \"Chapter Title\",\n",
    "    12: \"Section Number\",\n",
    "    14: \"Section Title\",\n",
    "    3: \"Article Number\",\n",
    "    4: \"Article Title\",\n",
    "    5: \"Item\",\n",
    "    6: \"Sub Item\",\n",
    "    7: \"Sub Sub Item\",\n",
    "    11: \"Ref Tables\",\n",
    "}\n",
    "\n",
    "# ─────────── Helper Functions ───────────\n",
    "def classify_text(text: str, tokenizer, model):\n",
    "    \"\"\"Classify a text segment using the trained model.\"\"\"\n",
    "    try:\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            logits = model(**encoding).logits\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf, pred = probs.max(dim=1)\n",
    "        return ID_TO_LABEL.get(pred.item(), \"Unknown\"), conf.item()\n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying text: {e}\")\n",
    "        return \"Unknown\", 0.0\n",
    "\n",
    "def build_json_structure(lines):\n",
    "    \"\"\"Build a hierarchical JSON structure from classified lines.\"\"\"\n",
    "    data = []\n",
    "    current = {\n",
    "        \"annex\": None, \"title\": None, \"chapter\": None, \"section\": None,\n",
    "        \"article\": None, \"item\": None, \"sub_item\": None, \"sub_sub_item\": None\n",
    "    }\n",
    "    for _, text, label, _ in lines:\n",
    "        if label == \"Trash\":\n",
    "            continue\n",
    "        if label == \"Annex Number\":\n",
    "            current[\"annex\"] = {\"type\": \"annex\", \"number\": text, \"title\": \"\", \"titles\": []}\n",
    "            data.append(current[\"annex\"])\n",
    "            current.update(title=None, chapter=None, section=None, article=None, item=None, sub_item=None, sub_sub_item=None)\n",
    "        elif label == \"Annex Title\" and current[\"annex\"]:\n",
    "            current[\"annex\"][\"title\"] = text\n",
    "        elif label == \"Title Number\" and current[\"annex\"]:\n",
    "            current[\"title\"] = {\"type\": \"title\", \"number\": text, \"title\": \"\", \"chapters\": [], \"articles\": []}\n",
    "            current[\"annex\"][\"titles\"].append(current[\"title\"])\n",
    "            current.update(chapter=None, section=None, article=None, item=None, sub_item=None, sub_sub_item=None)\n",
    "        elif label == \"Title Title\" and current[\"title\"]:\n",
    "            current[\"title\"][\"title\"] = text\n",
    "        elif label == \"Chapter Number\" and current[\"title\"]:\n",
    "            current[\"chapter\"] = {\"type\": \"chapter\", \"number\": text, \"title\": \"\", \"sections\": [], \"articles\": []}\n",
    "            current[\"title\"][\"chapters\"].append(current[\"chapter\"])\n",
    "            current.update(section=None, article=None, item=None, sub_item=None, sub_sub_item=None)\n",
    "        elif label == \"Chapter Title\" and current[\"chapter\"]:\n",
    "            current[\"chapter\"][\"title\"] = text\n",
    "        elif label == \"Section Number\" and current[\"chapter\"]:\n",
    "            current[\"section\"] = {\"type\": \"section\", \"number\": text, \"title\": \"\", \"articles\": []}\n",
    "            current[\"chapter\"][\"sections\"].append(current[\"section\"])\n",
    "            current.update(article=None, item=None, sub_item=None, sub_sub_item=None)\n",
    "        elif label == \"Section Title\" and current[\"section\"]:\n",
    "            current[\"section\"][\"title\"] = text\n",
    "        elif label == \"Article Number\" and (current[\"section\"] or current[\"chapter\"] or current[\"title\"]):\n",
    "            current[\"article\"] = {\"type\": \"article\", \"number\": text, \"title\": \"\", \"items\": []}\n",
    "            if current[\"section\"]:\n",
    "                current[\"section\"][\"articles\"].append(current[\"article\"])\n",
    "            elif current[\"chapter\"]:\n",
    "                current[\"chapter\"][\"articles\"].append(current[\"article\"])\n",
    "            else:\n",
    "                current[\"title\"][\"articles\"].append(current[\"article\"])\n",
    "            current.update(item=None, sub_item=None, sub_sub_item=None)\n",
    "        elif label == \"Article Title\" and current[\"article\"]:\n",
    "            current[\"article\"][\"title\"] = text\n",
    "        elif label == \"Item\" and current[\"article\"]:\n",
    "            current[\"item\"] = {\"type\": \"item\", \"text\": text, \"sub_items\": []}\n",
    "            current[\"article\"][\"items\"].append(current[\"item\"])\n",
    "            current.update(sub_item=None, sub_sub_item=None)\n",
    "        elif label == \"Sub Item\" and current[\"item\"]:\n",
    "            current[\"sub_item\"] = {\"type\": \"sub_item\", \"text\": text, \"sub_sub_items\": []}\n",
    "            current[\"item\"][\"sub_items\"].append(current[\"sub_item\"])\n",
    "            current[\"sub_sub_item\"] = None\n",
    "        elif label == \"Sub Sub Item\" and current[\"sub_item\"]:\n",
    "            current[\"sub_sub_item\"] = {\"type\": \"sub_sub_item\", \"text\": text}\n",
    "            current[\"sub_item\"][\"sub_sub_items\"].append(current[\"sub_sub_item\"])\n",
    "        elif label == \"Ref Tables\":\n",
    "            target = (\n",
    "                current[\"sub_sub_item\"] or current[\"sub_item\"] or current[\"item\"] or\n",
    "                current[\"article\"] or current[\"chapter\"] or current[\"title\"] or current[\"annex\"]\n",
    "            )\n",
    "            if target:\n",
    "                target.setdefault(\"table_refs\", []).append(text)\n",
    "        else:\n",
    "            target = (\n",
    "                current[\"sub_sub_item\"] or current[\"sub_item\"] or current[\"item\"] or\n",
    "                (current[\"article\"] if current[\"article\"] and not current[\"article\"][\"title\"] else None) or\n",
    "                (current[\"section\"] if current[\"section\"] and not current[\"section\"][\"title\"] else None) or\n",
    "                (current[\"chapter\"] if current[\"chapter\"] and not current[\"chapter\"][\"title\"] else None) or\n",
    "                (current[\"title\"] if current[\"title\"] and not current[\"title\"][\"title\"] else None) or\n",
    "                (current[\"annex\"] if current[\"annex\"] and not current[\"annex\"][\"title\"] else None)\n",
    "            )\n",
    "            if target:\n",
    "                field = \"text\" if \"text\" in target else \"title\"\n",
    "                target[field] += \" \" + text\n",
    "    return data\n",
    "\n",
    "# ─────────── GUI Class ───────────\n",
    "class ClassificationApp(tk.Tk):\n",
    "    \"\"\"GUI for reviewing and editing text classifications.\"\"\"\n",
    "    def __init__(self, classified_lines):\n",
    "        super().__init__()\n",
    "        self.title(\"Text Classification Review\")\n",
    "        self.geometry(\"1300x720\")\n",
    "        self.combobox_vars = []\n",
    "        self.checkbox_vars = []\n",
    "        self.text_vars = []\n",
    "\n",
    "        # Toolbar\n",
    "        toolbar = tk.Frame(self, bd=1, relief=\"raised\")\n",
    "        toolbar.pack(side=\"top\", fill=\"x\")\n",
    "        tk.Button(toolbar, text=\"Merge Selected Lines\", command=self.merge_selected).pack(side=\"left\", padx=6, pady=4)\n",
    "        tk.Button(toolbar, text=\"Confirm and Save JSON\", bg=\"#c3f5c3\", command=self.save_json).pack(side=\"right\", padx=6, pady=4)\n",
    "\n",
    "        # Scrollable Area\n",
    "        container = tk.Frame(self)\n",
    "        container.pack(fill=\"both\", expand=True)\n",
    "        self.canvas = tk.Canvas(container, highlightthickness=0)\n",
    "        self.canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        scrollbar = tk.Scrollbar(container, orient=\"vertical\", command=self.canvas.yview)\n",
    "        scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "        self.scrollable = tk.Frame(self.canvas)\n",
    "        self.canvas.create_window((0, 0), window=self.scrollable, anchor=\"nw\")\n",
    "        self.canvas.configure(yscrollcommand=scrollbar.set)\n",
    "        self.scrollable.bind(\"<Configure>\", lambda e: self.canvas.configure(scrollregion=self.canvas.bbox(\"all\")))\n",
    "        self.canvas.bind_all(\"<MouseWheel>\", lambda e: self.canvas.yview_scroll(int(-1 * (e.delta / 120)), \"units\"))\n",
    "\n",
    "        # Header\n",
    "        tk.Label(self.scrollable, text=\"✓\", width=2).grid(row=0, column=0)\n",
    "        tk.Label(self.scrollable, text=\"#\", width=4).grid(row=0, column=1)\n",
    "        tk.Label(self.scrollable, text=\"Text\", width=80, anchor=\"w\").grid(row=0, column=2)\n",
    "        tk.Label(self.scrollable, text=\"Label\", width=20).grid(row=0, column=3)\n",
    "        tk.Label(self.scrollable, text=\"Confidence\", width=10).grid(row=0, column=4)\n",
    "\n",
    "        # Populate rows\n",
    "        for row, (idx, text, label, conf) in enumerate(classified_lines, start=1):\n",
    "            self._add_line(row, idx, text, label, conf)\n",
    "\n",
    "    def _add_line(self, row, idx, text, label, conf):\n",
    "        \"\"\"Add a single classified line to the GUI.\"\"\"\n",
    "        var_sel = tk.BooleanVar()\n",
    "        tk.Checkbutton(self.scrollable, variable=var_sel).grid(row=row, column=0, padx=2)\n",
    "        self.checkbox_vars.append(var_sel)\n",
    "        tk.Label(self.scrollable, text=f\"{idx:03d}\", width=4).grid(row=row, column=1)\n",
    "        txt = tk.Text(self.scrollable, height=2, width=80, wrap=\"word\")\n",
    "        txt.insert(\"1.0\", text)\n",
    "        txt.grid(row=row, column=2, padx=5)\n",
    "        self.text_vars.append(txt)\n",
    "        var_lbl = tk.StringVar(value=label)\n",
    "        ttk.Combobox(\n",
    "            self.scrollable,\n",
    "            textvariable=var_lbl,\n",
    "            values=list(ID_TO_LABEL.values()),\n",
    "            state=\"readonly\",\n",
    "            width=20\n",
    "        ).grid(row=row, column=3, padx=5)\n",
    "        self.combobox_vars.append((idx, var_lbl, conf))\n",
    "        tk.Label(self.scrollable, text=f\"{conf:.2f}\").grid(row=row, column=4)\n",
    "\n",
    "    def _get_current_lines(self):\n",
    "        \"\"\"Retrieve current lines from the GUI.\"\"\"\n",
    "        lines = []\n",
    "        for (idx, lbl_var, conf), txt_widget in zip(self.combobox_vars, self.text_vars):\n",
    "            text = txt_widget.get(\"1.0\", \"end-1c\").strip()\n",
    "            lines.append((idx, text, lbl_var.get(), conf))\n",
    "        return lines\n",
    "\n",
    "    def _rebuild_grid(self, lines):\n",
    "        \"\"\"Rebuild the GUI grid with updated lines.\"\"\"\n",
    "        for widget in self.scrollable.grid_slaves():\n",
    "            if int(widget.grid_info()[\"row\"]) != 0:\n",
    "                widget.destroy()\n",
    "        self.checkbox_vars.clear()\n",
    "        self.text_vars.clear()\n",
    "        self.combobox_vars.clear()\n",
    "        for row, (idx, text, label, conf) in enumerate(lines, start=1):\n",
    "            self._add_line(row, idx, text, label, conf)\n",
    "\n",
    "    def merge_selected(self):\n",
    "        \"\"\"Merge selected lines into a single line.\"\"\"\n",
    "        selected = [i for i, var in enumerate(self.checkbox_vars) if var.get()]\n",
    "        if len(selected) < 2:\n",
    "            messagebox.showinfo(\"Merge Lines\", \"Select at least two lines to merge.\")\n",
    "            return\n",
    "        merged_text = \" \".join(self.text_vars[i].get(\"1.0\", \"end-1c\").strip() for i in selected)\n",
    "        first = selected[0]\n",
    "        self.text_vars[first].delete(\"1.0\", \"end\")\n",
    "        self.text_vars[first].insert(\"1.0\", merged_text)\n",
    "        for i in sorted(selected[1:], reverse=True):\n",
    "            del self.text_vars[i]\n",
    "            del self.checkbox_vars[i]\n",
    "            del self.combobox_vars[i]\n",
    "        for var in self.checkbox_vars:\n",
    "            var.set(False)\n",
    "        self._rebuild_grid(self._get_current_lines())\n",
    "\n",
    "    def save_json(self):\n",
    "        \"\"\"Save structured data to JSON and close the GUI.\"\"\"\n",
    "        try:\n",
    "            lines = self._get_current_lines()\n",
    "            structured_data = build_json_structure(lines)\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "            output_path = f\"{OUTPUT_JSON_BASE}_{timestamp}.json\"\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(structured_data, f, indent=4, ensure_ascii=False)\n",
    "            messagebox.showinfo(\"Success\", f\"✅ JSON saved to:\\n{output_path}\")\n",
    "            self.destroy()\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Failed to save JSON: {e}\")\n",
    "\n",
    "# ─────────── Main Execution ───────────\n",
    "def main():\n",
    "    \"\"\"Main function: classify document lines and launch GUI.\"\"\"\n",
    "    # Validate paths\n",
    "    if not os.path.exists(INPUT_DOCX_PATH):\n",
    "        print(f\"Error: Input document not found at {INPUT_DOCX_PATH}\")\n",
    "        return\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"Error: Model checkpoint not found at {MODEL_PATH}\")\n",
    "        return\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or tokenizer: {e}\")\n",
    "        return\n",
    "\n",
    "    # Read Word document\n",
    "    try:\n",
    "        doc = Document(INPUT_DOCX_PATH)\n",
    "        lines = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading document: {e}\")\n",
    "        return\n",
    "\n",
    "    # Classify lines\n",
    "    classified_lines = []\n",
    "    for idx, line in enumerate(lines, start=1):\n",
    "        label, conf = classify_text(line, tokenizer, model)\n",
    "        classified_lines.append((idx, line, label, conf))\n",
    "\n",
    "    # Launch GUI\n",
    "    try:\n",
    "        ClassificationApp(classified_lines).mainloop()\n",
    "    except Exception as e:\n",
    "        print(f\"Error launching GUI: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enriching Structured JSON with Table Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a cross-reference between the extracted tables in Tables.json and the references found in structured_output.json to enrich the structure with corresponding summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "# ─────────── Configuration ───────────\n",
    "# OpenAI API settings\n",
    "CLIENT = OpenAI(\n",
    "    api_key=\"ddc-temp-free-e3b73cd814cc4f3ea79b5d4437912663\",\n",
    "    base_url=\"https://api.devsdocode.com/v1\",\n",
    ")\n",
    "MODEL_NAME = \"provider-4/gpt-4.1\"\n",
    "\n",
    "# File paths\n",
    "STRUCTURED_OUTPUT_PATH = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\structured_output.json\"\n",
    "TABLES_DATA_PATH = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\extracted_tables\\Tables_clean_final_with_summary.json\"\n",
    "OUTPUT_PATH = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\structured_output_updated.json\"\n",
    "\n",
    "# ─────────── Utility Functions ───────────\n",
    "def load_json(file_path):\n",
    "    \"\"\"Load JSON data from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            return json.load(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    \"\"\"Save JSON data to a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON {file_path}: {e}\")\n",
    "\n",
    "def extract_table_refs(structured_output):\n",
    "    \"\"\"Extract unique table references from structured data.\"\"\"\n",
    "    refs = []\n",
    "    def recursive_search(struct):\n",
    "        if isinstance(struct, list):\n",
    "            for item in struct:\n",
    "                recursive_search(item)\n",
    "        elif isinstance(struct, dict):\n",
    "            if \"table_refs\" in struct:\n",
    "                refs.extend(struct[\"table_refs\"])\n",
    "            for key in struct:\n",
    "                recursive_search(struct[key])\n",
    "    recursive_search(structured_output)\n",
    "    return list(set(refs))\n",
    "\n",
    "# ─────────── Table Matching ───────────\n",
    "def match_tables_with_openai(table_refs, table_data):\n",
    "    \"\"\"Match table references to table numbers using OpenAI.\"\"\"\n",
    "    if not table_refs:\n",
    "        return {}\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in document analysis. Your task is to correctly associate table references\n",
    "(`table_refs`) from a structured document with actual table numbers (`table_number`)\n",
    "extracted from the document.\n",
    "\n",
    "Instructions:\n",
    "- Match table numbers directly when possible (e.g., \"TABLE I\" ↔ \"TABLE I\").\n",
    "- If the reference is descriptive (e.g., \"Fire resistance classification\"),\n",
    "  match it with the most relevant table title (`table_title`).\n",
    "- If multiple tables match a reference, return all of them.\n",
    "- If no match is found, return \"Unknown\".\n",
    "\n",
    "Table References to Match:\n",
    "{json.dumps(table_refs, indent=4)}\n",
    "\n",
    "Extracted Tables:\n",
    "{json.dumps([{t['table_number']: t['table_title']} for t in table_data], indent=4)}\n",
    "\n",
    "Expected JSON Output Format:\n",
    "{{\n",
    "    \"matches\": {{\n",
    "        \"TABLE REF 1\": [\"TABLE NUMBER 1\"],\n",
    "        \"TABLE REF 2\": [\"TABLE NUMBER 2\", \"TABLE NUMBER 3\"]\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "    max_retries = 5\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = CLIENT.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=2048,\n",
    "            )\n",
    "            response_text = response.choices[0].message.content.strip()\n",
    "            print(f\"\\n🔍 OpenAI Response:\\n{response_text}\")\n",
    "            cleaned_text = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            match = re.search(r'\"matches\"\\s*:\\s*({.*?})\\s*(\\n|$)', cleaned_text, re.DOTALL)\n",
    "            if not match:\n",
    "                print(\"⚠️ 'matches' block not found.\")\n",
    "                return {}\n",
    "            matches_str = match.group(1)\n",
    "            matches_str = re.sub(r',\\s*}', '}', matches_str)\n",
    "            matches_str = re.sub(r',\\s*]', ']', matches_str)\n",
    "            matches_str = re.sub(r'(?<=\\n)([^\\s\"{][^:]+?):', r'\"\\1\":', matches_str)\n",
    "            full_json = '{\"matches\": ' + matches_str + '}'\n",
    "            return json.loads(full_json).get(\"matches\", {})\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Attempt {attempt + 1} failed: {e}\")\n",
    "            time.sleep(10)\n",
    "    return {}\n",
    "\n",
    "# ─────────── Update Summaries ───────────\n",
    "def update_table_summaries(structured_output, table_matches, table_dict):\n",
    "    \"\"\"Update structured data with table summaries.\"\"\"\n",
    "    def recursive_update(struct):\n",
    "        if isinstance(struct, list):\n",
    "            for item in struct:\n",
    "                recursive_update(item)\n",
    "        elif isinstance(struct, dict):\n",
    "            if \"table_refs\" in struct:\n",
    "                summaries_native = []\n",
    "                summaries_english = []\n",
    "                for table_ref in struct[\"table_refs\"]:\n",
    "                    matched_tables = table_matches.get(table_ref, [])\n",
    "                    for table_number in matched_tables:\n",
    "                        if table_number.upper() in table_dict:\n",
    "                            table_info = table_dict[table_number.upper()]\n",
    "                            summaries_native.append(table_info.get(\"table_summary\", \"\"))\n",
    "                            summaries_english.append(table_info.get(\"table_summary_en\", \"\"))\n",
    "                            print(f\"✅ Linked: {table_ref} ↔ {table_number}\")\n",
    "                if summaries_native:\n",
    "                    struct[\"table_summary\"] = \" | \".join(summaries_native)\n",
    "                if summaries_english:\n",
    "                    struct[\"table_summary_en\"] = \" | \".join(summaries_english)\n",
    "            for key in struct:\n",
    "                recursive_update(struct[key])\n",
    "    recursive_update(structured_output)\n",
    "\n",
    "# ─────────── Main Execution ───────────\n",
    "def main():\n",
    "    \"\"\"Main function: match table references and update summaries.\"\"\"\n",
    "    # Validate input files\n",
    "    if not os.path.exists(STRUCTURED_OUTPUT_PATH):\n",
    "        print(f\"Error: Structured output JSON not found at {STRUCTURED_OUTPUT_PATH}\")\n",
    "        return\n",
    "    if not os.path.exists(TABLES_DATA_PATH):\n",
    "        print(f\"Error: Tables data JSON not found at {TABLES_DATA_PATH}\")\n",
    "        return\n",
    "\n",
    "    # Load data\n",
    "    structured_output = load_json(STRUCTURED_OUTPUT_PATH)\n",
    "    tables_data = load_json(TABLES_DATA_PATH)\n",
    "    table_dict = {table[\"table_number\"].upper(): table for table in tables_data}\n",
    "\n",
    "    # Extract and match table references\n",
    "    table_refs = extract_table_refs(structured_output)\n",
    "    matches = match_tables_with_openai(table_refs, tables_data)\n",
    "\n",
    "    # Update summaries\n",
    "    update_table_summaries(structured_output, matches, table_dict)\n",
    "\n",
    "    # Save updated JSON\n",
    "    save_json(structured_output, OUTPUT_PATH)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword Extractor for Ontology: Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script automatically extracts technical keywords from clauses within a structured JSON file. Using the API, sentences are processed in batches, and relevant terms are identified and directly associated with their original structures in the JSON. The result is semantic enrichment of the content, supporting ontology development or specialized document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "from openai import OpenAI\n",
    "\n",
    "# ============== CONFIGURATION ==============\n",
    "API_KEY = \"ddc-temp-free-e3b73cd814cc4f3ea79b5d4437912663\"\n",
    "BASE_URL = \"https://api.devsdocode.com/v1\"\n",
    "MODEL_NAME = \"provider-4/gpt-4.1\"\n",
    "REQUESTS_PER_MINUTE = 3\n",
    "MIN_INTERVAL_SECONDS = 60.0 / REQUESTS_PER_MINUTE\n",
    "MAX_RETRIES = 5\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "INPUT_PATH = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\structured_output_updated.json\"\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "# ============== JSON UTILITIES ==============\n",
    "\n",
    "def extract_json_array(content: str) -> Optional[str]:\n",
    "    start = content.find(\"[\")\n",
    "    end = content.rfind(\"]\") + 1\n",
    "    if start == -1 or end == 0 or end <= start:\n",
    "        return None\n",
    "    return content[start:end]\n",
    "\n",
    "def strip_code_fences(raw: str) -> str:\n",
    "    return re.sub(r\"```[a-zA-Z0-9_]*\\n?|```\", \"\", raw).strip()\n",
    "\n",
    "def fix_common_json_errors(text: str) -> str:\n",
    "    text = text.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\")\n",
    "    text = re.sub(r',\\s*}', '}', text)\n",
    "    text = re.sub(r',\\s*]', ']', text)\n",
    "    text = re.sub(r'\"([^\"]*?)\"\\s*\"([^\"]*?)\"', r'\"\\1\", \"\\2\"', text)  # merged strings\n",
    "    return text\n",
    "\n",
    "# ============== TRAVERSING STRUCTURE ==============\n",
    "\n",
    "def _collect_items_recursive(obj: Dict[str, Any], parent_text: str, sink: List[Dict]):\n",
    "    cur = obj.get(\"text\", \"\").strip()\n",
    "    combined = (parent_text + \" \" + cur).strip()\n",
    "\n",
    "    for key in (\"sub_item\", \"sub_items\", \"sub_sub_item\", \"sub_sub_items\"):\n",
    "        subs = obj.get(key, [])\n",
    "        if subs:\n",
    "            for sub in subs:\n",
    "                _collect_items_recursive(sub, combined, sink)\n",
    "            return\n",
    "\n",
    "    if not obj.get(\"palavras_chave\"):\n",
    "        sink.append({\"text\": combined, \"target\": obj})\n",
    "\n",
    "def collect_all_item_phrases(article_dict: Dict[str, Any], sink: List[Dict]):\n",
    "    _collect_items_recursive(article_dict, \"\", sink)\n",
    "\n",
    "def walk_articles(container: Dict[str, Any], sink: List[Dict]):\n",
    "    for article in container.get(\"articles\", []):\n",
    "        for item in article.get(\"items\", []):\n",
    "            collect_all_item_phrases(item, sink)\n",
    "    for key in (\"chapters\", \"sections\"):\n",
    "        for sub in container.get(key, []):\n",
    "            walk_articles(sub, sink)\n",
    "\n",
    "# ============== PROMPT AND LLM CALL ==============\n",
    "\n",
    "def build_messages(sentences: List[str]):\n",
    "    system_message = (\n",
    "        \"You are an assistant specialized in extracting only technical terms \"\n",
    "        \"related to fire safety in buildings and accessibility for an ontology.\\n\"\n",
    "        \"- For each sentence (can be in different languages), return relevant *technical keywords* \"\n",
    "        \"in the same language.\\n\"\n",
    "        \"- DO NOT include generic words (e.g., 'and', 'the') or legal references.\\n\"\n",
    "        \"- DO NOT output text before or after the JSON.\\n\"\n",
    "        \"- Return strictly a JSON array of objects in this format:\\n\"\n",
    "        \"[{\\\"text\\\": \\\"original text\\\", \\\"keywords\\\": [\\\"term1\\\", \\\"term2\\\"]}]\\n\"\n",
    "        \"- Strings must use double quotes (\\\") and commas must be correctly placed.\"\n",
    "    )\n",
    "\n",
    "    user_lines = [f\"{i+1}. {s}\" for i, s in enumerate(sentences)]\n",
    "    user_content = \"Sentences:\\n\" + \"\\n\".join(user_lines)\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "\n",
    "def call_llm(sentences: List[str]) -> Optional[List[Dict[str, Any]]]:\n",
    "    messages = build_messages(sentences)\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            start = time.time()\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=messages,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            print(f\"LLM responded in {time.time() - start:.1f}s (t={datetime.now().time()})\")\n",
    "            raw = strip_code_fences(resp.choices[0].message.content)\n",
    "            fixed = fix_common_json_errors(raw)\n",
    "            array_str = extract_json_array(fixed)\n",
    "            if array_str:\n",
    "                return json.loads(array_str)\n",
    "            print(\"⚠️ No JSON array extracted.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error on attempt {attempt}/{MAX_RETRIES}: {e}\")\n",
    "            time.sleep(5 * attempt)\n",
    "    return None\n",
    "\n",
    "# ============== EXECUTION ==============\n",
    "\n",
    "def main():\n",
    "    with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data: List[Dict[str, Any]] = json.load(f)\n",
    "\n",
    "    all_phrases: List[Dict[str, Any]] = []\n",
    "    for annex in data:\n",
    "        for title in annex.get(\"titles\", []):\n",
    "            walk_articles(title, all_phrases)\n",
    "\n",
    "    print(f\"Total sentences to process: {len(all_phrases)}\")\n",
    "    index = 0\n",
    "    total = len(all_phrases)\n",
    "    last_call = 0.0\n",
    "\n",
    "    while index < total:\n",
    "        elapsed = time.time() - last_call\n",
    "        wait = max(0, MIN_INTERVAL_SECONDS - elapsed)\n",
    "        if wait > 0:\n",
    "            print(f\"Waiting {wait:.1f}s to respect rate limit...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "        batch = all_phrases[index : index + BATCH_SIZE]\n",
    "        texts = [b[\"text\"] for b in batch]\n",
    "        print(f\"→ Batch {index//BATCH_SIZE + 1}: {len(texts)} sentences (#{index+1}–{index+len(texts)})\")\n",
    "\n",
    "        result = call_llm(texts)\n",
    "        last_call = time.time()\n",
    "\n",
    "        if result is None:\n",
    "            print(\"❌ Permanent failure while processing batch.\")\n",
    "            break\n",
    "\n",
    "        valid = min(len(result), len(batch))\n",
    "        if valid < len(batch):\n",
    "            print(f\"⚠️ Only {valid}/{len(batch)} objects were returned.\")\n",
    "\n",
    "        for i in range(valid):\n",
    "            batch[i][\"target\"][\"palavras_chave\"] = result[i].get(\"keywords\", [])\n",
    "\n",
    "        index += BATCH_SIZE\n",
    "        print(f\"✓ Progress: {index}/{total} ({index/total:.0%})\\n\")\n",
    "\n",
    "    with open(INPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"✅ Extraction complete. File updated at:\", INPUT_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated Definition Enrichment of Technical Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The script iterates through all the clauses in the structured JSON file, collects the previously extracted keywords, and, based on the legislation identified at the beginning of the methodology (from the results_gui.json file), uses the API to generate specific definitions for each term. These definitions are then directly associated with each keyword in the JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import re\n",
    "from openai import OpenAI\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# ─────────── GPT Configuration ───────────\n",
    "client = OpenAI(\n",
    "    api_key=\"ddc-temp-free-e3b73cd814cc4f3ea79b5d4437912663\",\n",
    "    base_url=\"https://api.devsdocode.com/v1\",\n",
    ")\n",
    "\n",
    "# ─────────── File Paths ───────────\n",
    "JSON_METADATA_PATH = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\Documento_Nativo_results_gui.json\"\n",
    "JSON_INPUT_PATH = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\structured_output_updated.json\"\n",
    "\n",
    "# ─────────── Metadata Extraction ───────────\n",
    "def extract_metadata_from_json(filepath):\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        country = data.get(\"country\", \"Unknown Country\")\n",
    "        lang_code = data.get(\"language\", \"pt\").lower()\n",
    "        lang_map = {\n",
    "            \"pt\": \"Portuguese\", \"en\": \"English\", \"es\": \"Spanish\", \"fr\": \"French\",\n",
    "            \"de\": \"German\", \"be\": \"Belgian\", \"it\": \"Italian\", \"nl\": \"Dutch\",\n",
    "            \"da\": \"Danish\", \"lt\": \"Lithuanian\"\n",
    "        }\n",
    "        lang = lang_map.get(lang_code, lang_code.capitalize())\n",
    "        selected_norms = data.get(\"selected_norms\", [])\n",
    "        legislation = selected_norms[0][\"name\"] if selected_norms and \"name\" in selected_norms[0] else \"Unknown Legislation\"\n",
    "\n",
    "        return country, lang, legislation\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load metadata: {e}\")\n",
    "        return \"Unknown\", \"Portuguese\", \"Unknown Legislation\"\n",
    "\n",
    "# ─────────── Keyword Collection ───────────\n",
    "def collect_keywords_from_items(items, seen, keywords):\n",
    "    for item in items:\n",
    "        for level in [item] + item.get(\"sub_items\", []) + sum([s.get(\"sub_sub_items\", []) for s in item.get(\"sub_items\", [])], []):\n",
    "            for kw in level.get(\"palavras_chave\", []):\n",
    "                if isinstance(kw, str) and kw not in seen:\n",
    "                    seen.add(kw)\n",
    "                    keywords.append(kw)\n",
    "\n",
    "def collect_all_keywords(data):\n",
    "    seen, keywords = set(), []\n",
    "    for annex in data:\n",
    "        for title in annex.get(\"titles\", []):\n",
    "            for article in title.get(\"articles\", []): collect_keywords_from_items(article.get(\"items\", []), seen, keywords)\n",
    "            for chapter in title.get(\"chapters\", []):\n",
    "                for article in chapter.get(\"articles\", []): collect_keywords_from_items(article.get(\"items\", []), seen, keywords)\n",
    "        for article in annex.get(\"articles\", []): collect_keywords_from_items(article.get(\"items\", []), seen, keywords)\n",
    "    return keywords\n",
    "\n",
    "# ─────────── GPT Definitions ───────────\n",
    "def get_definitions_gpt(keywords, legislation, lang, retries=3, delay=25):\n",
    "    prompt = f\"\"\"\n",
    "You are an expert in fire safety legislation. For each technical keyword listed below, return a JSON object where:\n",
    "- The key is the keyword.\n",
    "- The value is an object with two keys:\n",
    "  • \"{lang}\": a definition (max 100 words) starting with the capitalized keyword and 'é'.\n",
    "  • \"English\": a definition starting with the translated keyword and 'is'.\n",
    "Do not include code fences, extra text or comments. Only valid JSON.\n",
    "Context: {legislation}\n",
    "Keywords: {', '.join(keywords)}\n",
    "\"\"\".strip()\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"provider-4/gpt-4.1\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=1024,\n",
    "            )\n",
    "            text = response.choices[0].message.content.strip()\n",
    "            text = re.sub(r\"```json|```\", \"\", text).strip()\n",
    "            return json.loads(text)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "    return {kw: {lang: f\"{kw} é indefinido\", \"English\": f\"{kw} is undefined\"} for kw in keywords}\n",
    "\n",
    "# ─────────── Insert Definitions ───────────\n",
    "def insert_definitions(data, defs, lang):\n",
    "    def ins(items):\n",
    "        for item in items:\n",
    "            item[\"palavras_chave\"] = {kw: defs.get(kw, {lang: f\"{kw} é indefinido\", \"English\": f\"{kw} is undefined\"}) for kw in item.get(\"palavras_chave\", [])}\n",
    "            for sub in item.get(\"sub_items\", []):\n",
    "                sub[\"palavras_chave\"] = {kw: defs.get(kw, {lang: f\"{kw} é indefinido\", \"English\": f\"{kw} is undefined\"}) for kw in sub.get(\"palavras_chave\", [])}\n",
    "                for subsub in sub.get(\"sub_sub_items\", []):\n",
    "                    subsub[\"palavras_chave\"] = {kw: defs.get(kw, {lang: f\"{kw} é indefinido\", \"English\": f\"{kw} is undefined\"}) for kw in subsub.get(\"palavras_chave\", [])}\n",
    "\n",
    "    for annex in data:\n",
    "        for title in annex.get(\"titles\", []):\n",
    "            for article in title.get(\"articles\", []): ins(article.get(\"items\", []))\n",
    "            for chapter in title.get(\"chapters\", []):\n",
    "                for article in chapter.get(\"articles\", []): ins(article.get(\"items\", []))\n",
    "        for article in annex.get(\"articles\", []): ins(article.get(\"items\", []))\n",
    "\n",
    "# ─────────── Main Execution ───────────\n",
    "# Main execution: reads file, queries GPT, starts GUI\n",
    "def main():\n",
    "    country, lang, legislation = extract_metadata_from_json(JSON_METADATA_PATH)\n",
    "    print(f\"🌍 Country: {country} | Language: {lang} | Legislation: {legislation}\")\n",
    "\n",
    "    with open(JSON_INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    keywords = collect_all_keywords(data)\n",
    "    print(f\"🔍 Total unique keywords: {len(keywords)}\")\n",
    "\n",
    "    defs = {}\n",
    "    batch_size = 6\n",
    "    for i in range(0, len(keywords), batch_size):\n",
    "        batch = keywords[i:i+batch_size]\n",
    "        print(f\"➡️  Processing batch {i//batch_size+1} with {len(batch)} keywords\")\n",
    "        defs.update(get_definitions_gpt(batch, legislation, lang))\n",
    "        if i + batch_size < len(keywords):\n",
    "            time.sleep(60)  # respect rate limit\n",
    "\n",
    "    insert_definitions(data, defs, lang)\n",
    "    data.append({\"extracted_metadata\": {\"country\": country, \"language\": lang, \"legislation\": legislation}})\n",
    "\n",
    "    with open(JSON_INPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    print(\"✅ File updated with definitions and metadata.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "# Main execution: reads file, queries GPT, starts GUI\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment of Identifiers to Each Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script processes a structured JSON file and assigns unique hierarchical identifiers (id, id_T) to each element, including annexes, titles, chapters, articles, and paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "###############################\n",
    "#     FILES AND CONFIG        #\n",
    "###############################\n",
    "JSON_INPUT_FILE = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\structured_output_updated.json\"\n",
    "JSON_OUTPUT_FILE = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\structured_output_prepared.json\"\n",
    "\n",
    "COUNTRY_MAP = {\n",
    "    \"Portugal\": \"PT\",\n",
    "    \"Belgium\": \"BE\",\n",
    "    \"Netherlands\": \"NL\",\n",
    "    \"Denmark\": \"DK\",\n",
    "    \"Lithuania\": \"LT\",\n",
    "}\n",
    "\n",
    "###############################\n",
    "# Helper Functions\n",
    "###############################\n",
    "def short_title_id(full_str: str) -> str:\n",
    "    text = full_str.strip()\n",
    "    m = re.match(r'^(Título|Capítulo)\\s+([IVXLCDM]+)$', text, re.IGNORECASE)\n",
    "    if m:\n",
    "        word = m.group(1).lower()\n",
    "        roman = m.group(2).upper()\n",
    "        return \"Tit\" + roman if word.startswith(\"título\") else \"Cap\" + roman\n",
    "    return re.sub(r\"\\s+\", \"\", text)\n",
    "\n",
    "def detect_country_code(json_data):\n",
    "    default_code = \"PT\"\n",
    "    if isinstance(json_data, list) and json_data:\n",
    "        last_item = json_data[-1]\n",
    "        if isinstance(last_item, dict) and \"extracted_metadata\" in last_item:\n",
    "            c = last_item[\"extracted_metadata\"].get(\"country\", \"\").strip()\n",
    "            return COUNTRY_MAP.get(c, default_code)\n",
    "    return default_code\n",
    "\n",
    "def create_annex_id(index, cc):\n",
    "    return f\"{cc}-Ane{index}\"\n",
    "\n",
    "def create_article_id(parent_id, article_str):\n",
    "    match = re.search(r'(\\d+)', article_str)\n",
    "    num = match.group(1) if match else article_str.replace(\" \", \"\")\n",
    "    return f\"{parent_id}_Art{num}\"\n",
    "\n",
    "def extract_letter(text):\n",
    "    m = re.match(r'^\\s*([a-zA-Z])\\)', text)\n",
    "    return m.group(1).lower() if m else None\n",
    "\n",
    "def extract_roman_lowercase(text):\n",
    "    m = re.match(r'^\\s*((?:i+|v|x+))\\)', text.strip(), re.IGNORECASE)\n",
    "    return m.group(1).lower() if m else None\n",
    "\n",
    "###############################\n",
    "# parse_items (com id e id_T)\n",
    "###############################\n",
    "def parse_items(items, parent_id, level=0):\n",
    "    for i, item in enumerate(items, start=1):\n",
    "        text_str = item.get(\"text\", \"\").strip()\n",
    "\n",
    "        # Definir o tipo de identificador\n",
    "        if level == 0:\n",
    "            m = re.match(r'^(\\d+)', text_str)\n",
    "            item_num = m.group(1) if m else str(i)\n",
    "            local_id = f\"Ite{item_num}\"\n",
    "        elif level == 1:\n",
    "            letter = extract_letter(text_str)\n",
    "            local_id = f\"Sub{letter or i}\"\n",
    "        elif level == 2:\n",
    "            roman = extract_roman_lowercase(text_str)\n",
    "            local_id = f\"SSub{roman or i}\"\n",
    "        else:\n",
    "            local_id = f\"Ite{i}\"\n",
    "\n",
    "        item_id = f\"{parent_id}_{local_id}\"\n",
    "        item[\"id\"] = item_id\n",
    "\n",
    "        # Campos padrão obrigatórios\n",
    "        item.setdefault(\"type\", \"item\" if level == 0 else \"sub_item\" if level == 1 else \"sub_sub_item\")\n",
    "        item.setdefault(\"id_T\", [])\n",
    "\n",
    "        # Processar table_refs e gerar id_T\n",
    "        if \"table_refs\" in item and item[\"table_refs\"]:\n",
    "            id_t_list = []\n",
    "            for table_ref in item[\"table_refs\"]:\n",
    "                match = re.search(r'QUADRO\\s+([IVXLCDM]+)', table_ref, re.IGNORECASE)\n",
    "                if match:\n",
    "                    table_id = f\"Tab{match.group(1).upper()}\"\n",
    "                    id_t_list.append(f\"{item_id}_{table_id}\")\n",
    "            item[\"id_T\"] = id_t_list\n",
    "\n",
    "        # Processar subníveis\n",
    "        if \"sub_items\" in item:\n",
    "            item.setdefault(\"sub_items\", [])\n",
    "            parse_items(item[\"sub_items\"], item_id, level + 1)\n",
    "\n",
    "        if \"sub_sub_items\" in item:\n",
    "            item.setdefault(\"sub_sub_items\", [])\n",
    "            parse_items(item[\"sub_sub_items\"], item_id, level + 2)\n",
    "\n",
    "    return items\n",
    "\n",
    "###############################\n",
    "# parse_articles\n",
    "###############################\n",
    "def parse_articles(articles, parent_id):\n",
    "    for article in articles:\n",
    "        art_id = create_article_id(parent_id, article[\"number\"])\n",
    "        article[\"id\"] = art_id\n",
    "        article.setdefault(\"type\", \"article\")\n",
    "        if \"items\" in article:\n",
    "            parse_items(article[\"items\"], art_id, 0)\n",
    "\n",
    "###############################\n",
    "# parse_titles\n",
    "###############################\n",
    "def parse_titles(titles, annex_id):\n",
    "    for title in titles:\n",
    "        short_id = short_title_id(title[\"number\"])\n",
    "        title_id = f\"{annex_id}_{short_id}\"\n",
    "        title[\"id\"] = title_id\n",
    "        title.setdefault(\"type\", \"title\")\n",
    "\n",
    "        if \"chapters\" in title:\n",
    "            for chapter in title[\"chapters\"]:\n",
    "                ch_short = short_title_id(chapter[\"number\"])\n",
    "                chap_id = f\"{title_id}_{ch_short}\"\n",
    "                chapter[\"id\"] = chap_id\n",
    "                chapter.setdefault(\"type\", \"chapter\")\n",
    "                chapter.setdefault(\"sections\", [])\n",
    "                if \"articles\" in chapter:\n",
    "                    parse_articles(chapter[\"articles\"], chap_id)\n",
    "\n",
    "        if \"articles\" in title:\n",
    "            parse_articles(title[\"articles\"], title_id)\n",
    "\n",
    "###################\n",
    "# process_json\n",
    "###################\n",
    "def process_json(json_data):\n",
    "    cc = detect_country_code(json_data)\n",
    "    annex_index = 0\n",
    "    for annex in json_data:\n",
    "        if \"extracted_metadata\" in annex:\n",
    "            continue\n",
    "        annex_id = create_annex_id(annex_index, cc)\n",
    "        annex[\"id\"] = annex_id\n",
    "        annex.setdefault(\"type\", \"annex\")\n",
    "        annex_index += 1\n",
    "\n",
    "        if \"titles\" in annex:\n",
    "            parse_titles(annex[\"titles\"], annex_id)\n",
    "\n",
    "        if \"articles\" in annex:\n",
    "            parse_articles(annex[\"articles\"], annex_id)\n",
    "\n",
    "    return json_data\n",
    "\n",
    "###################\n",
    "# MAIN\n",
    "###################\n",
    "if __name__ == \"__main__\":\n",
    "    with open(JSON_INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed = process_json(data)\n",
    "\n",
    "    with open(JSON_OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(processed, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"✅ JSON processed and saved to '{JSON_OUTPUT_FILE}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract EN keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Mapeamento país → idioma\n",
    "COUNTRY_LANG_MAP = {\n",
    "    \"Portugal\": \"pt\",\n",
    "    \"Belgium\": \"nl\",\n",
    "    \"Netherlands\": \"nl\",\n",
    "    \"Denmark\": \"da\",\n",
    "    \"Lithuania\": \"lt\"\n",
    "}\n",
    "\n",
    "def extract_keyword_from_definition(english_def: str) -> str:\n",
    "    \"\"\"Extrai a keyword antes de 'is' ou 'are', limpando parênteses.\"\"\"\n",
    "    match = re.match(r'^([\\w\\s\\-/&()]+?)\\s+(is|are)\\b', english_def)\n",
    "    if match:\n",
    "        raw_kw = match.group(1).strip()\n",
    "        cleaned_kw = re.sub(r'\\s*\\([^)]*\\)', '', raw_kw)\n",
    "        return cleaned_kw.strip().lower()\n",
    "    return english_def.split()[0].lower()  # fallback\n",
    "\n",
    "def process_node(node, lang_tag=\"pt\", remove_palavras_chave=False):\n",
    "    if \"palavras_chave\" in node:\n",
    "        keywords_main = []\n",
    "        keywords_en = []\n",
    "        definitions_main = []\n",
    "        definitions_en = []\n",
    "\n",
    "        for main_kw, entry in node[\"palavras_chave\"].items():\n",
    "            def_main = \"\"\n",
    "            if lang_tag == \"pt\":\n",
    "                def_main = entry.get(\"Portuguese\", \"\")\n",
    "            elif lang_tag == \"nl\":\n",
    "                def_main = entry.get(\"Dutch\", \"\")\n",
    "            elif lang_tag == \"da\":\n",
    "                def_main = entry.get(\"Danish\", \"\")\n",
    "            elif lang_tag == \"lt\":\n",
    "                def_main = entry.get(\"Lithuanian\", \"\")\n",
    "            def_en = entry.get(\"English\", \"\").strip()\n",
    "\n",
    "            if def_main:\n",
    "                keywords_main.append(main_kw.strip())\n",
    "                definitions_main.append(def_main.strip())\n",
    "            if def_en:\n",
    "                keywords_en.append(extract_keyword_from_definition(def_en))\n",
    "                definitions_en.append(def_en.strip())\n",
    "\n",
    "        if keywords_main:\n",
    "            node[f\"keywords_{lang_tag}\"] = keywords_main\n",
    "            node[f\"definitions_{lang_tag}\"] = definitions_main\n",
    "        if keywords_en:\n",
    "            node[\"keywords_en\"] = keywords_en\n",
    "            node[\"definitions_en\"] = definitions_en\n",
    "\n",
    "        if remove_palavras_chave:\n",
    "            del node[\"palavras_chave\"]\n",
    "\n",
    "    # Recursivamente processa os subnós\n",
    "    for key in [\"titles\", \"chapters\", \"articles\", \"items\", \"sub_items\", \"sub_sub_items\", \"sections\", \"paragraphs\"]:\n",
    "        if key in node:\n",
    "            for child in node[key]:\n",
    "                process_node(child, lang_tag, remove_palavras_chave)\n",
    "\n",
    "def run(input_path: str, output_path: str, metadata_path: str, remove_original=False):\n",
    "    # Verificar se os ficheiros existem\n",
    "    for path in [input_path, metadata_path]:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"❌ Ficheiro não encontrado: {path}\")\n",
    "            return\n",
    "\n",
    "    # Obter país e idioma a partir do ficheiro de metadados\n",
    "    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "        country = metadata.get(\"country\", \"Portugal\")\n",
    "        lang_tag = COUNTRY_LANG_MAP.get(country, \"pt\")  # padrão: pt\n",
    "\n",
    "    # Carregar o ficheiro principal\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for annex in data:\n",
    "        process_node(annex, lang_tag=lang_tag, remove_palavras_chave=remove_original)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"✅ JSON atualizado salvo em: {output_path} (idioma: {lang_tag})\")\n",
    "\n",
    "# Caminhos de ficheiro\n",
    "input_file = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\structured_output_prepared.json\"\n",
    "output_file = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\structured_output_Protege.json\"\n",
    "metadata_file = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\Documento_Nativo_results_gui.json\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run(input_file, output_file, metadata_file, remove_original=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of OWL Individuals from Structured JSON for Ontology Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script processes a structured JSON file containing legal clauses and converts each element—such as annexes, titles, chapters, articles, and paragraphs—into OWL individuals in Turtle (.ttl) format. It assigns unique identifiers to each node (:hasID), extracts either the original text or description (:hasOriginalText or :hasDescription), and establishes hierarchical relationships among elements (e.g., :ArticleOf, :hasChpater). It also includes annotations with keywords and their definitions (:hasKeyword, :hasDefiniton). Finally, all generated individuals are appended to an existing base .ttl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# ─────────── Global Variables ───────────\n",
    "# Dictionary to store hasTable relations\n",
    "TABLE_RELATIONS = {}\n",
    "\n",
    "# Dictionary to store all individuals before generating the TTL\n",
    "INDIVIDUALS = {}\n",
    "\n",
    "# ─────────── Language Mapping ───────────\n",
    "COUNTRY_LANG_MAP= {\n",
    "    \"Portugal\": \"pt\",\n",
    "    \"Belgium\": \"nl\",      # ajuste para \"fr\" ou \"de\" se preferir\n",
    "    \"Dennmark\": \"da\",\n",
    "    \"Netherlands\": \"nl\",\n",
    "    \"Lithuania\": \"lt\"\n",
    "}\n",
    "\n",
    "lang_tag = \"und\"  # Default value\n",
    "\n",
    "\n",
    "# ─────────── Helper Functions ───────────\n",
    "def extract_ancestors_from_id(node_id):\n",
    "    \"\"\"Extract all ancestor IDs from a node ID.\"\"\"\n",
    "    parts = node_id.split('_')\n",
    "    ancestors = []\n",
    "    current_id = \"\"\n",
    "    for i, part in enumerate(parts):\n",
    "        if i > 0:\n",
    "            current_id += \"_\"\n",
    "        current_id += part\n",
    "        ancestors.append(current_id)\n",
    "    return ancestors[:-1]  # Exclude the node itself\n",
    "\n",
    "def sanitize_text(text):\n",
    "    \"\"\"Sanitize text for TTL output by removing invalid characters.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace('\"\"\"', '\"').replace('\"', \"'\")\n",
    "    return text.strip()\n",
    "\n",
    "def collect_all_ids_in_hierarchy(node):\n",
    "    \"\"\"Recursively collect IDs of all hierarchical elements.\"\"\"\n",
    "    result = {\n",
    "        \"titles\": [],\n",
    "        \"chapters\": [],\n",
    "        \"articles\": [],\n",
    "        \"items\": [],\n",
    "        \"sub_items\": [],\n",
    "        \"sub_sub_items\": [],\n",
    "        \"sections\": [],\n",
    "        \"paragraphs\": [],\n",
    "        \"annexes\": []\n",
    "    }\n",
    "\n",
    "    # Process titles\n",
    "    if \"titles\" in node:\n",
    "        for title in node[\"titles\"]:\n",
    "            result[\"titles\"].append(title[\"id\"])\n",
    "            child_info = collect_all_ids_in_hierarchy(title)\n",
    "            for key in result:\n",
    "                result[key].extend(child_info[key])\n",
    "\n",
    "    # Process chapters\n",
    "    if \"chapters\" in node:\n",
    "        for chapter in node[\"chapters\"]:\n",
    "            result[\"chapters\"].append(chapter[\"id\"])\n",
    "            child_info = collect_all_ids_in_hierarchy(chapter)\n",
    "            for key in result:\n",
    "                result[key].extend(child_info[key])\n",
    "\n",
    "    # Process articles\n",
    "    if \"articles\" in node:\n",
    "        for article in node[\"articles\"]:\n",
    "            result[\"articles\"].append(article[\"id\"])\n",
    "            child_info = collect_all_ids_in_hierarchy(article)\n",
    "            for key in result:\n",
    "                result[key].extend(child_info[key])\n",
    "\n",
    "    # Process items\n",
    "    if \"items\" in node:\n",
    "        for item in node[\"items\"]:\n",
    "            result[\"items\"].append(item[\"id\"])\n",
    "            child_info = collect_all_ids_in_hierarchy(item)\n",
    "            for key in result:\n",
    "                result[key].extend(child_info[key])\n",
    "\n",
    "    # Process sub-items\n",
    "    if \"sub_items\" in node:\n",
    "        for sub_item in node[\"sub_items\"]:\n",
    "            result[\"sub_items\"].append(sub_item[\"id\"])\n",
    "            child_info = collect_all_ids_in_hierarchy(sub_item)\n",
    "            for key in result:\n",
    "                result[key].extend(child_info[key])\n",
    "\n",
    "    # Process sub-sub-items\n",
    "    if \"sub_sub_items\" in node:\n",
    "        for sub_sub_item in node[\"sub_sub_items\"]:\n",
    "            result[\"sub_sub_items\"].append(sub_sub_item[\"id\"])\n",
    "            child_info = collect_all_ids_in_hierarchy(sub_sub_item)\n",
    "            for key in result:\n",
    "                result[key].extend(child_info[key])\n",
    "\n",
    "    # Process sections\n",
    "    if \"sections\" in node:\n",
    "        for section in node[\"sections\"]:\n",
    "            result[\"sections\"].append(section[\"id\"])\n",
    "            child_info = collect_all_ids_in_hierarchy(section)\n",
    "            for key in result:\n",
    "                result[key].extend(child_info[key])\n",
    "\n",
    "    # Process paragraphs\n",
    "    if \"paragraphs\" in node:\n",
    "        for paragraph in node[\"paragraphs\"]:\n",
    "            result[\"paragraphs\"].append(paragraph[\"id\"])\n",
    "            child_info = collect_all_ids_in_hierarchy(paragraph)\n",
    "            for key in result:\n",
    "                result[key].extend(child_info[key])\n",
    "\n",
    "    # Process annex\n",
    "    if \"type\" in node and node[\"type\"] == \"annex\":\n",
    "        result[\"annexes\"].append(node[\"id\"])\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_individual_ttl(node, node_class, country, ancestors=None):\n",
    "    \"\"\"Generate TTL for an individual node.\"\"\"\n",
    "    if ancestors is None:\n",
    "        ancestors = {}\n",
    "\n",
    "    lines = []\n",
    "    node_id = node[\"id\"]\n",
    "    print(f\"Processing node: {node_id} ({node_class})\")\n",
    "\n",
    "    # Define the individual as owl:NamedIndividual\n",
    "    lines.append(f\":{node_id} a owl:NamedIndividual , :{node_class} ;\")\n",
    "    lines.append(f'    :hasID \"{node_id}\" ;')\n",
    "\n",
    "    # Add data properties\n",
    "    if node_class in [\"Item\", \"SubItem\", \"SubSubItem\"]:\n",
    "        original_text = sanitize_text(node.get(\"text\", \"\"))\n",
    "        lines.append(f'    :hasOriginalText \"{original_text}\" ;')\n",
    "    else:\n",
    "        description = sanitize_text(node.get(\"title\", \"\"))\n",
    "        lines.append(f'    :hasDescription \"{description}\" ;')\n",
    "\n",
    "    lines.append(f'    :hasCountry \"{country}\" ;')\n",
    "    designator = node.get(\"type\", \"\")\n",
    "    lines.append(f'    :hasDesignator \"{designator}\"')\n",
    "\n",
    "    lines_to_add = []\n",
    "\n",
    "    # Add hierarchical ancestry relations\n",
    "    parent_id = extract_ancestors_from_id(node_id)[-1] if extract_ancestors_from_id(node_id) else None\n",
    "    if parent_id:\n",
    "        if node_class == \"Title\":\n",
    "            lines_to_add.append(f\":TitleOf :{parent_id}\")\n",
    "        elif node_class == \"Chapter\":\n",
    "            lines_to_add.append(f\":ChapterOf :{parent_id}\")\n",
    "        elif node_class == \"Article\":\n",
    "            lines_to_add.append(f\":ArticleOf :{parent_id}\")\n",
    "        elif node_class == \"Item\":\n",
    "            lines_to_add.append(f\":ItemOf :{parent_id}\")\n",
    "        elif node_class == \"SubItem\":\n",
    "            lines_to_add.append(f\":SubItemOf :{parent_id}\")\n",
    "        elif node_class == \"SubSubItem\":\n",
    "            lines_to_add.append(f\":SubSubItemOf :{parent_id}\")\n",
    "\n",
    "    # Add hierarchical relations\n",
    "    hierarchy_info = collect_all_ids_in_hierarchy(node)\n",
    "    if hierarchy_info[\"titles\"]:\n",
    "        titles_list = \", \".join(f\":{tid}\" for tid in hierarchy_info[\"titles\"])\n",
    "        lines_to_add.append(f\":hasTitle {titles_list}\")\n",
    "    if hierarchy_info[\"chapters\"]:\n",
    "        chapters_list = \", \".join(f\":{cid}\" for cid in hierarchy_info[\"chapters\"])\n",
    "        lines_to_add.append(f\":hasChapter {chapters_list}\")\n",
    "    if hierarchy_info[\"articles\"]:\n",
    "        articles_list = \", \".join(f\":{aid}\" for aid in hierarchy_info[\"articles\"])\n",
    "        lines_to_add.append(f\":hasArticle {articles_list}\")\n",
    "    if hierarchy_info[\"items\"]:\n",
    "        items_list = \", \".join(f\":{item_id}\" for item_id in hierarchy_info[\"items\"])\n",
    "        lines_to_add.append(f\":hasItem {items_list}\")\n",
    "    if hierarchy_info[\"sub_items\"]:\n",
    "        subitems_list = \", \".join(f\":{subid}\" for subid in hierarchy_info[\"sub_items\"])\n",
    "        lines_to_add.append(f\":hasSubItem {subitems_list}\")\n",
    "    if hierarchy_info[\"sub_sub_items\"]:\n",
    "        subsubitems_list = \", \".join(f\":{subsubid}\" for subsubid in hierarchy_info[\"sub_sub_items\"])\n",
    "        lines_to_add.append(f\":hasSubSubItem {subsubitems_list}\")\n",
    "    if \"id_S\" in node and node[\"id_S\"]:\n",
    "        sections_list = \", \".join(f\":{sid}\" for sid in node[\"id_S\"])\n",
    "        lines_to_add.append(f\":hasSection {sections_list}\")\n",
    "    if \"id_P\" in node and node[\"id_P\"]:\n",
    "        paragraphs_list = \", \".join(f\":{pid}\" for pid in node[\"id_P\"])\n",
    "        lines_to_add.append(f\":hasParagraph {paragraphs_list}\")\n",
    "\n",
    "    # Maintain hasTable relations from id_T\n",
    "    if \"id_T\" in node and node[\"id_T\"]:\n",
    "        tables_list = \", \".join(f\":{tid}\" for tid in node[\"id_T\"])\n",
    "        lines_to_add.append(f\":hasTable {tables_list}\")\n",
    "        print(f\"Adding original hasTable for {node_id}: {tables_list}\")\n",
    "\n",
    "    # ✅ Add keywords and definitions from the new structure\n",
    "    if \"keywords_pt\" in node:\n",
    "        for kw in node[\"keywords_pt\"]:\n",
    "            lines_to_add.append(f':hasKeyword \"{sanitize_text(kw)}\"@{lang_tag}')\n",
    "    if \"keywords_en\" in node:\n",
    "        for kw in node[\"keywords_en\"]:\n",
    "            lines_to_add.append(f':hasKeyword \"{sanitize_text(kw)}\"@en')\n",
    "    if \"definitions_pt\" in node:\n",
    "        for definition in node[\"definitions_pt\"]:\n",
    "            lines_to_add.append(f':hasDefinition \"{sanitize_text(definition)}\"@{lang_tag}')\n",
    "    if \"definitions_en\" in node:\n",
    "        for definition in node[\"definitions_en\"]:\n",
    "            lines_to_add.append(f':hasDefinition \"{sanitize_text(definition)}\"@en')\n",
    "\n",
    "    # Store lines in individuals dictionary\n",
    "    INDIVIDUALS[node_id] = {\"lines\": lines, \"lines_to_add\": lines_to_add}\n",
    "    return lines\n",
    "\n",
    "\n",
    "def generate_table_ttl(table_id, table_ref, table_summary, country, ancestors=None):\n",
    "    \"\"\"Generate TTL for a table individual.\"\"\"\n",
    "    if ancestors is None:\n",
    "        ancestors = {}\n",
    "\n",
    "    print(f\"Generating table: {table_id}\")\n",
    "    table_summary = sanitize_text(table_summary)\n",
    "    lines = []\n",
    "    lines.append(f\":{table_id} a owl:NamedIndividual , :Table ;\")\n",
    "    lines.append(f'    :hasID \"{table_id}\" ;')\n",
    "    lines.append(f'    :hasCountry \"{country}\" ;')\n",
    "    lines.append(f'    :hasDesignator \"Table\" ;')\n",
    "    lines.append(f'    :hasOriginalText \"{table_summary}\"')\n",
    "\n",
    "    # Extract ancestors from table ID\n",
    "    ancestor_ids = extract_ancestors_from_id(table_id)\n",
    "    print(f\"Ancestors extracted for {table_id}: {ancestor_ids}\")\n",
    "\n",
    "    # Add table to ancestors in TABLE_RELATIONS\n",
    "    for ancestor_id in ancestor_ids:\n",
    "        if ancestor_id not in TABLE_RELATIONS:\n",
    "            TABLE_RELATIONS[ancestor_id] = []\n",
    "        if table_id not in TABLE_RELATIONS[ancestor_id]:\n",
    "            TABLE_RELATIONS[ancestor_id].append(table_id)\n",
    "            print(f\"Adding hasTable for {ancestor_id}: {table_id}\")\n",
    "\n",
    "    lines_to_add = []\n",
    "    if \"annex\" in ancestors:\n",
    "        lines_to_add.append(f\":AnnexOf :{ancestors['annex']}\")\n",
    "    if \"title\" in ancestors:\n",
    "        lines_to_add.append(f\":TitleOf :{ancestors['title']}\")\n",
    "    if \"chapter\" in ancestors:\n",
    "        lines_to_add.append(f\":ChapterOf :{ancestors['chapter']}\")\n",
    "    if \"article\" in ancestors:\n",
    "        lines_to_add.append(f\":ArticleOf :{ancestors['article']}\")\n",
    "    if \"item\" in ancestors:\n",
    "        lines_to_add.append(f\":ItemOf :{ancestors['item']}\")\n",
    "    if \"sub_item\" in ancestors:\n",
    "        lines_to_add.append(f\":SubItemOf :{ancestors['sub_item']}\")\n",
    "    if \"sub_sub_item\" in ancestors:\n",
    "        lines_to_add.append(f\":SubSubItemOf :{ancestors['sub_sub_item']}\")\n",
    "\n",
    "    INDIVIDUALS[table_id] = {\"lines\": lines, \"lines_to_add\": lines_to_add}\n",
    "    return lines\n",
    "\n",
    "# ─────────── Process Hierarchy ───────────\n",
    "def process_annex(node, country):\n",
    "    \"\"\"Process an annex and its children.\"\"\"\n",
    "    lines = []\n",
    "    annex_id = node[\"id\"]\n",
    "    ancestors = {\"annex\": annex_id}\n",
    "    lines.extend(generate_individual_ttl(node, \"Annex\", country, ancestors))\n",
    "\n",
    "    for title in node.get(\"titles\", []):\n",
    "        lines.extend(process_title(title, country, ancestors))\n",
    "    return lines\n",
    "\n",
    "def process_title(node, country, ancestors=None):\n",
    "    \"\"\"Process a title and its children.\"\"\"\n",
    "    if ancestors is None:\n",
    "        ancestors = {}\n",
    "    lines = []\n",
    "    title_id = node[\"id\"]\n",
    "    new_ancestors = ancestors.copy()\n",
    "    new_ancestors[\"title\"] = title_id\n",
    "    lines.extend(generate_individual_ttl(node, \"Title\", country, new_ancestors))\n",
    "\n",
    "    for chapter in node.get(\"chapters\", []):\n",
    "        lines.extend(process_chapter(chapter, country, new_ancestors))\n",
    "    for article in node.get(\"articles\", []):\n",
    "        lines.extend(process_article(article, country, new_ancestors))\n",
    "    return lines\n",
    "\n",
    "def process_chapter(node, country, ancestors=None):\n",
    "    \"\"\"Process a chapter and its children.\"\"\"\n",
    "    if ancestors is None:\n",
    "        ancestors = {}\n",
    "    lines = []\n",
    "    chapter_id = node[\"id\"]\n",
    "    new_ancestors = ancestors.copy()\n",
    "    new_ancestors[\"chapter\"] = chapter_id\n",
    "    lines.extend(generate_individual_ttl(node, \"Chapter\", country, new_ancestors))\n",
    "\n",
    "    for article in node.get(\"articles\", []):\n",
    "        lines.extend(process_article(article, country, new_ancestors))\n",
    "    return lines\n",
    "\n",
    "def process_article(node, country, ancestors=None):\n",
    "    \"\"\"Process an article and its children.\"\"\"\n",
    "    if ancestors is None:\n",
    "        ancestors = {}\n",
    "    lines = []\n",
    "    article_id = node[\"id\"]\n",
    "    new_ancestors = ancestors.copy()\n",
    "    new_ancestors[\"article\"] = article_id\n",
    "    lines.extend(generate_individual_ttl(node, \"Article\", country, new_ancestors))\n",
    "\n",
    "    for item in node.get(\"items\", []):\n",
    "        lines.extend(process_item(item, country, new_ancestors))\n",
    "    return lines\n",
    "\n",
    "def process_item(node, country, ancestors=None):\n",
    "    \"\"\"Process an item and its children.\"\"\"\n",
    "    if ancestors is None:\n",
    "        ancestors = {}\n",
    "    lines = []\n",
    "    item_id = node[\"id\"]\n",
    "    new_ancestors = ancestors.copy()\n",
    "    new_ancestors[\"item\"] = item_id\n",
    "    lines.extend(generate_individual_ttl(node, \"Item\", country, new_ancestors))\n",
    "\n",
    "    if \"id_T\" in node and node[\"id_T\"]:\n",
    "        for i, table_id in enumerate(node[\"id_T\"]):\n",
    "            table_ref = node.get(\"table_refs\", [f\"TABLE {i+1}\"])[i] if \"table_refs\" in node else f\"Table for {table_id}\"\n",
    "            table_summary = node.get(\"table_summary\", \"No summary available\")\n",
    "            lines.extend(generate_table_ttl(table_id, table_ref, table_summary, country, new_ancestors))\n",
    "\n",
    "    for sub_item in node.get(\"sub_items\", []):\n",
    "        lines.extend(process_subitem(sub_item, country, new_ancestors))\n",
    "    return lines\n",
    "\n",
    "def process_subitem(node, country, ancestors=None):\n",
    "    \"\"\"Process a sub-item and its children.\"\"\"\n",
    "    if ancestors is None:\n",
    "        ancestors = {}\n",
    "    lines = []\n",
    "    sub_item_id = node[\"id\"]\n",
    "    new_ancestors = ancestors.copy()\n",
    "    new_ancestors[\"sub_item\"] = sub_item_id\n",
    "    lines.extend(generate_individual_ttl(node, \"SubItem\", country, new_ancestors))\n",
    "\n",
    "    if \"id_T\" in node and node[\"id_T\"]:\n",
    "        for i, table_id in enumerate(node[\"id_T\"]):\n",
    "            table_ref = node.get(\"table_refs\", [f\"TABLE {i+1}\"])[i] if \"table_refs\" in node else f\"Table for {table_id}\"\n",
    "            table_summary = node.get(\"table_summary\", \"No summary available\")\n",
    "            lines.extend(generate_table_ttl(table_id, table_ref, table_summary, country, new_ancestors))\n",
    "\n",
    "    for sub_sub_item in node.get(\"sub_sub_items\", []):\n",
    "        lines.extend(process_subsubitem(sub_sub_item, country, new_ancestors))\n",
    "    return lines\n",
    "\n",
    "def process_subsubitem(node, country, ancestors=None):\n",
    "    \"\"\"Process a sub-sub-item and its children.\"\"\"\n",
    "    if ancestors is None:\n",
    "        ancestors = {}\n",
    "    lines = []\n",
    "    sub_sub_item_id = node[\"id\"]\n",
    "    new_ancestors = ancestors.copy()\n",
    "    new_ancestors[\"sub_sub_item\"] = sub_sub_item_id\n",
    "    lines.extend(generate_individual_ttl(node, \"SubSubItem\", country, new_ancestors))\n",
    "\n",
    "    if \"id_T\" in node and node[\"id_T\"]:\n",
    "        for i, table_id in enumerate(node[\"id_T\"]):\n",
    "            table_ref = node.get(\"table_refs\", [f\"TABLE {i+1}\"])[i] if \"table_refs\" in node else f\"Table for {table_id}\"\n",
    "            table_summary = node.get(\"table_summary\", \"No summary available\")\n",
    "            lines.extend(generate_table_ttl(table_id, table_ref, table_summary, country, new_ancestors))\n",
    "    return lines\n",
    "\n",
    "# ─────────── Finalize Individuals ───────────\n",
    "def finalize_individuals():\n",
    "    \"\"\"Finalize TTL lines for all individuals.\"\"\"\n",
    "    all_ttl_lines = []\n",
    "    for node_id, data in INDIVIDUALS.items():\n",
    "        lines = data[\"lines\"]\n",
    "        lines_to_add = data[\"lines_to_add\"].copy()\n",
    "\n",
    "        # Add hasTable relations from TABLE_RELATIONS\n",
    "        if node_id in TABLE_RELATIONS:\n",
    "            existing_tables = set()\n",
    "            for line in lines_to_add:\n",
    "                if line.startswith(\":hasTable\"):\n",
    "                    existing_tables.update(line.split(\":hasTable \")[1].split(\", \"))\n",
    "            new_tables = [f\":{tid}\" for tid in TABLE_RELATIONS[node_id] if f\":{tid}\" not in existing_tables]\n",
    "            if new_tables:\n",
    "                tables_list = \", \".join(new_tables)\n",
    "                lines_to_add.append(f\":hasTable {tables_list}\")\n",
    "                print(f\"Finalizing {node_id} with additional hasTable: {tables_list}\")\n",
    "\n",
    "        # Format lines with proper TTL syntax\n",
    "        if lines_to_add:\n",
    "            lines[-1] += \";\"\n",
    "            for i, triple_part in enumerate(lines_to_add):\n",
    "                if i < len(lines_to_add) - 1:\n",
    "                    lines.append(f\"    {triple_part} ;\")\n",
    "                else:\n",
    "                    lines.append(f\"    {triple_part} .\\n\")\n",
    "        else:\n",
    "            lines[-1] += \".\\n\"\n",
    "\n",
    "        all_ttl_lines.extend(lines)\n",
    "    return all_ttl_lines\n",
    "\n",
    "# ─────────── Main Execution ───────────\n",
    "def main():\n",
    "    \"\"\"Main function: read JSON, generate TTL individuals, and append to file.\"\"\"\n",
    "    # File paths\n",
    "    metadata_json_path = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\Documento_Nativo_results_gui.json\"\n",
    "    json_path = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\structured_output_Protege.json\"\n",
    "    ttl_base_path = r\"C:\\Users\\DEC_User\\Desktop\\FIREBIM\\rase_llm_project\\data\\Ontologia\\FRO-25022025_PT.ttl\"\n",
    "\n",
    "    # Check if files exist\n",
    "    if not os.path.exists(metadata_json_path):\n",
    "        print(f\"Error: Metadata JSON file {metadata_json_path} not found.\")\n",
    "        return\n",
    "    if not os.path.exists(json_path):\n",
    "        print(f\"Error: JSON file {json_path} not found.\")\n",
    "        return\n",
    "    if not os.path.exists(ttl_base_path):\n",
    "        print(f\"Error: Base TTL file {ttl_base_path} not found.\")\n",
    "        return\n",
    "\n",
    "    # Load country from metadata JSON\n",
    "    with open(metadata_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "        global lang_tag\n",
    "        country = metadata.get(\"country\", \"Unknown\")\n",
    "        lang_tag = COUNTRY_LANG_MAP.get(country, \"und\")\n",
    "\n",
    "\n",
    "    # Load JSON data\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Reset global variables\n",
    "    global TABLE_RELATIONS, INDIVIDUALS\n",
    "    TABLE_RELATIONS = {}\n",
    "    INDIVIDUALS = {}\n",
    "\n",
    "    # Process annexes\n",
    "    for annex in data:\n",
    "        if \"extracted_metadata\" in annex:\n",
    "            continue\n",
    "        process_annex(annex, country)\n",
    "\n",
    "    # Finalize and write TTL\n",
    "    all_ttl_lines = finalize_individuals()\n",
    "\n",
    "    with open(ttl_base_path, \"a\", encoding=\"utf-8\") as out_file:\n",
    "        out_file.write(\"\\n\\n########################################################\\n\")\n",
    "        out_file.write(\"# Individuals generated automatically from JSON\\n\")\n",
    "        out_file.write(\"########################################################\\n\\n\")\n",
    "        for line in all_ttl_lines:\n",
    "            out_file.write(line)\n",
    "\n",
    "    print(f\"✅ Individuals successfully appended to {ttl_base_path}!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FIREBIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
